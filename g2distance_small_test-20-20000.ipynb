{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25aa328c-fbde-4c86-837d-b109eac25828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ModuleList, Embedding\n",
    "from torch.nn import Sequential, ReLU, Linear\n",
    "from torch.nn import CrossEntropyLoss, MSELoss, L1Loss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, PNAConv, BatchNorm, global_add_pool\n",
    "\n",
    "from phylognn_model import G2Dist_GCNConv_Small\n",
    "\n",
    "from gene_graph_dataset import GeneGraphDataset\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1afe07e-f45d-4435-9efd-f7be458f83ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p, test_p = 0.7, 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c8f1633-bee2-464b-93e2-6fea1a471948",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GeneGraphDataset('dataset', 20, 20, graph_num = 1000)\n",
    "data_size = len(dataset)\n",
    "train_size, test_size = (int)(data_size * train_p), (int)(data_size * test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e2f694a-7ae1-455e-9582-8b7eb9ed67ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a95dcce3-4aaf-4d68-b153-a9581899a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle()\n",
    "train_dataset = dataset[:train_size]\n",
    "test_dataset = dataset[train_size:(train_size + test_size)]\n",
    "val_dataset = dataset[(train_size + test_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63f6283c-d51a-4747-8f1b-f437ca523abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_dataset), len(test_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddbf1dce-6f12-4fdd-be53-37271e03d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89c85b3a-1bdf-49db-8494-d04d29ba39eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_loader), len(test_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6c79a8d-bb5d-4875-80ff-6c22ece243e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = G2Dist_GCNConv_Small().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay = 0.0001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10,\n",
    "                              min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e364bfa2-7153-493c-9d74-ba7b1ae117ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = MSELoss()\n",
    "# l1_fn = L1Loss()\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "def train(train_loader):\n",
    "    model.train()\n",
    "\n",
    "    total_loss, counter = 0, 0\n",
    "    size = len(train_loader)\n",
    "    for batch, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        #loss = (out.squeeze() - data.y).abs().sum()\n",
    "        pred, y = out.softmax(axis = 1).argmax(axis = 1), data.y\n",
    "        counter += (pred == y).sum().item()\n",
    "        \n",
    "        loss = loss_fn(out, data.y)\n",
    "        \n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return total_loss / len(train_loader), counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce1b9829-772b-4e9b-a75e-1e9904d80afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_error, counter = 0, 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        \n",
    "        pred, y = out.softmax(axis = 1).argmax(axis = 1), data.y\n",
    "        counter += (pred == y).sum().item()\n",
    "        \n",
    "        # total_error += (out.squeeze() - data.y).abs().sum().item()\n",
    "        \n",
    "        total_error += loss_fn(out, data.y).item()\n",
    "        \n",
    "    return total_error / len(loader), counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fba2c20-6dee-4b91-acdb-8ee1fa2319fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='runs_g2d_10/g2dist_0020_0020_20000-small-run1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bb307-e484-493c-a60e-69911d93d554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan  1 23:26:34 2022\tEpoch: 001, Loss: 6.5481, Val: 69.6604, Test: 70.7975\n",
      "\t\t -- train_counter: 882, test_counter:191\n",
      "Sat Jan  1 23:26:50 2022\tEpoch: 002, Loss: 2.5768, Val: 85.8999, Test: 86.4222\n",
      "\t\t -- train_counter: 2197, test_counter:191\n",
      "Sat Jan  1 23:27:06 2022\tEpoch: 003, Loss: 2.3407, Val: 18.4953, Test: 18.5329\n",
      "\t\t -- train_counter: 2618, test_counter:211\n",
      "Sat Jan  1 23:27:23 2022\tEpoch: 004, Loss: 2.0855, Val: 4.5157, Test: 4.5748\n",
      "\t\t -- train_counter: 3222, test_counter:222\n",
      "Sat Jan  1 23:27:39 2022\tEpoch: 005, Loss: 1.9791, Val: 8.7308, Test: 8.6836\n",
      "\t\t -- train_counter: 3431, test_counter:279\n",
      "Sat Jan  1 23:27:56 2022\tEpoch: 006, Loss: 1.8337, Val: 11.1299, Test: 11.0914\n",
      "\t\t -- train_counter: 3923, test_counter:168\n",
      "Sat Jan  1 23:28:13 2022\tEpoch: 007, Loss: 1.7120, Val: 15.1981, Test: 15.1588\n",
      "\t\t -- train_counter: 4590, test_counter:52\n",
      "Sat Jan  1 23:28:29 2022\tEpoch: 008, Loss: 1.6798, Val: 14.5409, Test: 14.5897\n",
      "\t\t -- train_counter: 4671, test_counter:269\n",
      "Sat Jan  1 23:28:46 2022\tEpoch: 009, Loss: 1.6807, Val: 13.0270, Test: 13.2083\n",
      "\t\t -- train_counter: 4715, test_counter:145\n",
      "Sat Jan  1 23:29:02 2022\tEpoch: 010, Loss: 1.6077, Val: 15.6472, Test: 15.8152\n",
      "\t\t -- train_counter: 5031, test_counter:146\n",
      "Sat Jan  1 23:29:18 2022\tEpoch: 011, Loss: 1.5718, Val: 17.4520, Test: 17.5531\n",
      "\t\t -- train_counter: 5213, test_counter:141\n",
      "Sat Jan  1 23:29:35 2022\tEpoch: 012, Loss: 1.5750, Val: 15.3437, Test: 15.3873\n",
      "\t\t -- train_counter: 5139, test_counter:75\n",
      "Sat Jan  1 23:29:50 2022\tEpoch: 013, Loss: 1.5388, Val: 12.0890, Test: 12.1680\n",
      "\t\t -- train_counter: 5395, test_counter:173\n",
      "Sat Jan  1 23:30:07 2022\tEpoch: 014, Loss: 1.4765, Val: 22.2166, Test: 22.3566\n",
      "\t\t -- train_counter: 5735, test_counter:144\n",
      "Sat Jan  1 23:30:24 2022\tEpoch: 015, Loss: 1.4716, Val: 11.7442, Test: 11.7134\n",
      "\t\t -- train_counter: 5826, test_counter:181\n",
      "Sat Jan  1 23:30:39 2022\tEpoch: 016, Loss: 1.4516, Val: 11.7237, Test: 11.7726\n",
      "\t\t -- train_counter: 5903, test_counter:156\n",
      "Sat Jan  1 23:30:55 2022\tEpoch: 017, Loss: 1.4050, Val: 9.5261, Test: 9.5558\n",
      "\t\t -- train_counter: 6127, test_counter:151\n",
      "Sat Jan  1 23:31:11 2022\tEpoch: 018, Loss: 1.4119, Val: 11.2170, Test: 11.2257\n",
      "\t\t -- train_counter: 6160, test_counter:138\n",
      "Sat Jan  1 23:31:27 2022\tEpoch: 019, Loss: 1.3306, Val: 8.7958, Test: 8.7723\n",
      "\t\t -- train_counter: 6612, test_counter:181\n",
      "Sat Jan  1 23:31:44 2022\tEpoch: 020, Loss: 1.3104, Val: 7.9746, Test: 7.9138\n",
      "\t\t -- train_counter: 6726, test_counter:219\n",
      "Sat Jan  1 23:31:59 2022\tEpoch: 021, Loss: 1.4695, Val: 8.4207, Test: 8.3951\n",
      "\t\t -- train_counter: 6043, test_counter:396\n",
      "Sat Jan  1 23:32:16 2022\tEpoch: 022, Loss: 1.3623, Val: 8.1133, Test: 8.0943\n",
      "\t\t -- train_counter: 6465, test_counter:380\n",
      "Sat Jan  1 23:32:33 2022\tEpoch: 023, Loss: 1.3425, Val: 6.7720, Test: 6.8634\n",
      "\t\t -- train_counter: 6620, test_counter:356\n",
      "Sat Jan  1 23:32:49 2022\tEpoch: 024, Loss: 1.2550, Val: 3.1544, Test: 3.1293\n",
      "\t\t -- train_counter: 7050, test_counter:672\n",
      "Sat Jan  1 23:33:06 2022\tEpoch: 025, Loss: 1.3246, Val: 4.5718, Test: 4.5012\n",
      "\t\t -- train_counter: 6697, test_counter:556\n",
      "Sat Jan  1 23:33:23 2022\tEpoch: 026, Loss: 1.3708, Val: 2.8066, Test: 2.7789\n",
      "\t\t -- train_counter: 6609, test_counter:749\n",
      "Sat Jan  1 23:33:38 2022\tEpoch: 027, Loss: 1.3172, Val: 3.8158, Test: 3.7759\n",
      "\t\t -- train_counter: 6814, test_counter:680\n",
      "Sat Jan  1 23:33:55 2022\tEpoch: 028, Loss: 1.1913, Val: 6.4141, Test: 6.4152\n",
      "\t\t -- train_counter: 7543, test_counter:292\n",
      "Sat Jan  1 23:34:11 2022\tEpoch: 029, Loss: 1.1726, Val: 4.1717, Test: 4.0649\n",
      "\t\t -- train_counter: 7639, test_counter:649\n",
      "Sat Jan  1 23:34:28 2022\tEpoch: 030, Loss: 1.0974, Val: 4.2796, Test: 4.2279\n",
      "\t\t -- train_counter: 8060, test_counter:489\n",
      "Sat Jan  1 23:34:45 2022\tEpoch: 031, Loss: 1.1491, Val: 4.5497, Test: 4.5436\n",
      "\t\t -- train_counter: 7755, test_counter:581\n",
      "Sat Jan  1 23:35:01 2022\tEpoch: 032, Loss: 1.0608, Val: 3.5041, Test: 3.5429\n",
      "\t\t -- train_counter: 8332, test_counter:689\n",
      "Sat Jan  1 23:35:18 2022\tEpoch: 033, Loss: 1.0112, Val: 2.8435, Test: 2.8246\n",
      "\t\t -- train_counter: 8462, test_counter:887\n",
      "Sat Jan  1 23:35:35 2022\tEpoch: 034, Loss: 0.9696, Val: 6.9181, Test: 6.9481\n",
      "\t\t -- train_counter: 8814, test_counter:415\n",
      "Sat Jan  1 23:35:51 2022\tEpoch: 035, Loss: 1.0643, Val: 5.9179, Test: 5.8633\n",
      "\t\t -- train_counter: 8361, test_counter:629\n",
      "Sat Jan  1 23:36:08 2022\tEpoch: 036, Loss: 0.9886, Val: 8.9902, Test: 9.0342\n",
      "\t\t -- train_counter: 8717, test_counter:426\n",
      "Sat Jan  1 23:36:24 2022\tEpoch: 037, Loss: 0.8907, Val: 6.9226, Test: 6.9579\n",
      "\t\t -- train_counter: 9252, test_counter:422\n",
      "Sat Jan  1 23:36:41 2022\tEpoch: 038, Loss: 0.9112, Val: 4.4887, Test: 4.3891\n",
      "\t\t -- train_counter: 9254, test_counter:658\n",
      "Sat Jan  1 23:36:58 2022\tEpoch: 039, Loss: 1.1144, Val: 9.6000, Test: 9.4976\n",
      "\t\t -- train_counter: 8198, test_counter:443\n",
      "Sat Jan  1 23:37:14 2022\tEpoch: 040, Loss: 0.9758, Val: 7.1601, Test: 7.1090\n",
      "\t\t -- train_counter: 8751, test_counter:371\n",
      "Sat Jan  1 23:37:30 2022\tEpoch: 041, Loss: 1.1360, Val: 3.3399, Test: 3.2968\n",
      "\t\t -- train_counter: 8269, test_counter:964\n",
      "Sat Jan  1 23:37:47 2022\tEpoch: 042, Loss: 0.9534, Val: 3.5069, Test: 3.4303\n",
      "\t\t -- train_counter: 8901, test_counter:1006\n",
      "Sat Jan  1 23:38:03 2022\tEpoch: 043, Loss: 0.7917, Val: 10.2147, Test: 10.1369\n",
      "\t\t -- train_counter: 9905, test_counter:264\n",
      "Sat Jan  1 23:38:20 2022\tEpoch: 044, Loss: 0.7894, Val: 3.2069, Test: 3.1758\n",
      "\t\t -- train_counter: 9846, test_counter:1144\n",
      "Sat Jan  1 23:38:36 2022\tEpoch: 045, Loss: 0.8643, Val: 5.3210, Test: 5.2290\n",
      "\t\t -- train_counter: 9371, test_counter:762\n",
      "Sat Jan  1 23:38:54 2022\tEpoch: 046, Loss: 0.7150, Val: 6.4243, Test: 6.4872\n",
      "\t\t -- train_counter: 10268, test_counter:448\n",
      "Sat Jan  1 23:39:13 2022\tEpoch: 047, Loss: 0.6161, Val: 6.7799, Test: 6.7313\n",
      "\t\t -- train_counter: 10938, test_counter:343\n",
      "Sat Jan  1 23:39:31 2022\tEpoch: 048, Loss: 0.5680, Val: 6.5503, Test: 6.4453\n",
      "\t\t -- train_counter: 11153, test_counter:471\n",
      "Sat Jan  1 23:39:49 2022\tEpoch: 049, Loss: 0.6655, Val: 4.3564, Test: 4.1581\n",
      "\t\t -- train_counter: 10514, test_counter:1090\n",
      "Sat Jan  1 23:40:07 2022\tEpoch: 050, Loss: 0.6319, Val: 8.9134, Test: 8.7540\n",
      "\t\t -- train_counter: 10736, test_counter:572\n",
      "Sat Jan  1 23:40:24 2022\tEpoch: 051, Loss: 0.5801, Val: 3.9403, Test: 3.8116\n",
      "\t\t -- train_counter: 10984, test_counter:965\n",
      "Sat Jan  1 23:40:42 2022\tEpoch: 052, Loss: 0.5570, Val: 3.8851, Test: 3.7300\n",
      "\t\t -- train_counter: 11198, test_counter:1032\n",
      "Sat Jan  1 23:41:01 2022\tEpoch: 053, Loss: 0.5511, Val: 6.2910, Test: 6.1385\n",
      "\t\t -- train_counter: 11167, test_counter:812\n",
      "Sat Jan  1 23:41:19 2022\tEpoch: 054, Loss: 1.6205, Val: 39.7106, Test: 39.6036\n",
      "\t\t -- train_counter: 8006, test_counter:198\n",
      "Sat Jan  1 23:41:38 2022\tEpoch: 055, Loss: 1.1870, Val: 36.8637, Test: 36.4487\n",
      "\t\t -- train_counter: 7861, test_counter:198\n",
      "Sat Jan  1 23:41:57 2022\tEpoch: 056, Loss: 0.8213, Val: 12.9342, Test: 13.0827\n",
      "\t\t -- train_counter: 9717, test_counter:323\n",
      "Sat Jan  1 23:42:15 2022\tEpoch: 057, Loss: 0.7123, Val: 9.6204, Test: 9.4377\n",
      "\t\t -- train_counter: 10483, test_counter:279\n",
      "Sat Jan  1 23:42:33 2022\tEpoch: 058, Loss: 0.7531, Val: 4.0365, Test: 3.8546\n",
      "\t\t -- train_counter: 9968, test_counter:1148\n",
      "Sat Jan  1 23:42:51 2022\tEpoch: 059, Loss: 0.7069, Val: 4.0832, Test: 3.9777\n",
      "\t\t -- train_counter: 10266, test_counter:842\n",
      "Sat Jan  1 23:43:09 2022\tEpoch: 060, Loss: 0.5365, Val: 5.4039, Test: 5.2831\n",
      "\t\t -- train_counter: 11356, test_counter:583\n",
      "Sat Jan  1 23:43:28 2022\tEpoch: 061, Loss: 0.7391, Val: 18.0779, Test: 17.9645\n",
      "\t\t -- train_counter: 10426, test_counter:348\n",
      "Sat Jan  1 23:43:46 2022\tEpoch: 062, Loss: 0.8013, Val: 6.6035, Test: 6.3548\n",
      "\t\t -- train_counter: 9758, test_counter:710\n",
      "Sat Jan  1 23:44:03 2022\tEpoch: 063, Loss: 0.6867, Val: 4.0104, Test: 4.0128\n",
      "\t\t -- train_counter: 10378, test_counter:865\n",
      "Sat Jan  1 23:44:20 2022\tEpoch: 064, Loss: 0.4973, Val: 5.9859, Test: 5.9038\n",
      "\t\t -- train_counter: 11494, test_counter:650\n",
      "Sat Jan  1 23:44:38 2022\tEpoch: 065, Loss: 0.3945, Val: 9.7624, Test: 9.6013\n",
      "\t\t -- train_counter: 12236, test_counter:252\n",
      "Sat Jan  1 23:44:56 2022\tEpoch: 066, Loss: 0.3457, Val: 5.9278, Test: 5.7637\n",
      "\t\t -- train_counter: 12477, test_counter:622\n",
      "Sat Jan  1 23:45:14 2022\tEpoch: 067, Loss: 0.3146, Val: 4.8606, Test: 4.7156\n",
      "\t\t -- train_counter: 12664, test_counter:866\n",
      "Sat Jan  1 23:45:33 2022\tEpoch: 068, Loss: 0.4016, Val: 11.5160, Test: 11.2349\n",
      "\t\t -- train_counter: 12006, test_counter:532\n",
      "Sat Jan  1 23:45:51 2022\tEpoch: 069, Loss: 0.3293, Val: 5.2609, Test: 5.1483\n",
      "\t\t -- train_counter: 12418, test_counter:920\n",
      "Sat Jan  1 23:46:09 2022\tEpoch: 070, Loss: 0.2807, Val: 4.3824, Test: 4.2641\n",
      "\t\t -- train_counter: 12785, test_counter:1051\n",
      "Sat Jan  1 23:46:26 2022\tEpoch: 071, Loss: 0.2599, Val: 4.2461, Test: 4.1865\n",
      "\t\t -- train_counter: 12896, test_counter:1149\n",
      "Sat Jan  1 23:46:45 2022\tEpoch: 072, Loss: 0.2423, Val: 4.6631, Test: 4.6721\n",
      "\t\t -- train_counter: 12970, test_counter:1077\n",
      "Sat Jan  1 23:47:04 2022\tEpoch: 073, Loss: 0.2599, Val: 4.8095, Test: 4.6260\n",
      "\t\t -- train_counter: 12840, test_counter:1063\n",
      "Sat Jan  1 23:47:21 2022\tEpoch: 074, Loss: 0.7451, Val: 77.3114, Test: 77.4766\n",
      "\t\t -- train_counter: 11146, test_counter:292\n",
      "Sat Jan  1 23:47:40 2022\tEpoch: 075, Loss: 1.0942, Val: 24.5470, Test: 24.3314\n",
      "\t\t -- train_counter: 9014, test_counter:350\n",
      "Sat Jan  1 23:47:58 2022\tEpoch: 076, Loss: 0.7125, Val: 4.5743, Test: 4.3901\n",
      "\t\t -- train_counter: 10371, test_counter:942\n",
      "Sat Jan  1 23:48:16 2022\tEpoch: 077, Loss: 0.5207, Val: 8.8175, Test: 8.6810\n",
      "\t\t -- train_counter: 11457, test_counter:383\n",
      "Sat Jan  1 23:48:34 2022\tEpoch: 078, Loss: 0.4186, Val: 9.9400, Test: 9.6471\n",
      "\t\t -- train_counter: 11964, test_counter:323\n",
      "Sat Jan  1 23:48:52 2022\tEpoch: 079, Loss: 0.3286, Val: 6.4975, Test: 6.2423\n",
      "\t\t -- train_counter: 12540, test_counter:669\n",
      "Sat Jan  1 23:49:11 2022\tEpoch: 080, Loss: 0.2900, Val: 9.1863, Test: 8.8988\n",
      "\t\t -- train_counter: 12705, test_counter:361\n",
      "Sat Jan  1 23:49:30 2022\tEpoch: 081, Loss: 0.4394, Val: 4.6294, Test: 4.5400\n",
      "\t\t -- train_counter: 11859, test_counter:973\n",
      "Sat Jan  1 23:49:48 2022\tEpoch: 082, Loss: 0.4388, Val: 6.7640, Test: 6.5355\n",
      "\t\t -- train_counter: 11673, test_counter:867\n",
      "Sat Jan  1 23:50:07 2022\tEpoch: 083, Loss: 0.3305, Val: 5.6642, Test: 5.3975\n",
      "\t\t -- train_counter: 12369, test_counter:1074\n",
      "Sat Jan  1 23:50:25 2022\tEpoch: 084, Loss: 0.2893, Val: 6.0247, Test: 5.9256\n",
      "\t\t -- train_counter: 12569, test_counter:783\n",
      "Sat Jan  1 23:50:44 2022\tEpoch: 085, Loss: 0.2203, Val: 6.8751, Test: 6.7375\n",
      "\t\t -- train_counter: 13054, test_counter:1007\n",
      "Sat Jan  1 23:51:04 2022\tEpoch: 086, Loss: 0.1710, Val: 10.5407, Test: 10.3764\n",
      "\t\t -- train_counter: 13376, test_counter:423\n",
      "Sat Jan  1 23:51:23 2022\tEpoch: 087, Loss: 0.1658, Val: 5.1436, Test: 5.1012\n",
      "\t\t -- train_counter: 13372, test_counter:1184\n",
      "Sat Jan  1 23:51:41 2022\tEpoch: 088, Loss: 0.1537, Val: 9.8806, Test: 9.7211\n",
      "\t\t -- train_counter: 13410, test_counter:544\n",
      "Sat Jan  1 23:52:00 2022\tEpoch: 089, Loss: 0.1190, Val: 6.8338, Test: 6.7295\n",
      "\t\t -- train_counter: 13641, test_counter:1039\n",
      "Sat Jan  1 23:52:19 2022\tEpoch: 090, Loss: 0.1060, Val: 11.3702, Test: 11.1840\n",
      "\t\t -- train_counter: 13696, test_counter:415\n",
      "Sat Jan  1 23:52:38 2022\tEpoch: 091, Loss: 0.1385, Val: 8.2001, Test: 7.9609\n",
      "\t\t -- train_counter: 13485, test_counter:962\n",
      "Sat Jan  1 23:52:58 2022\tEpoch: 092, Loss: 0.6195, Val: 28.1935, Test: 27.8368\n",
      "\t\t -- train_counter: 11167, test_counter:378\n",
      "Sat Jan  1 23:53:16 2022\tEpoch: 093, Loss: 0.6052, Val: 16.2515, Test: 15.9847\n",
      "\t\t -- train_counter: 11053, test_counter:441\n",
      "Sat Jan  1 23:53:35 2022\tEpoch: 094, Loss: 0.4206, Val: 6.4098, Test: 6.3792\n",
      "\t\t -- train_counter: 11925, test_counter:781\n",
      "Sat Jan  1 23:53:53 2022\tEpoch: 095, Loss: 0.4477, Val: 6.6594, Test: 6.5280\n",
      "\t\t -- train_counter: 11718, test_counter:810\n",
      "Sat Jan  1 23:54:13 2022\tEpoch: 096, Loss: 0.2774, Val: 5.2484, Test: 5.1388\n",
      "\t\t -- train_counter: 12625, test_counter:1147\n",
      "Sat Jan  1 23:54:32 2022\tEpoch: 097, Loss: 0.1908, Val: 5.0300, Test: 4.8866\n",
      "\t\t -- train_counter: 13165, test_counter:1123\n",
      "Sat Jan  1 23:54:50 2022\tEpoch: 098, Loss: 0.1565, Val: 9.9413, Test: 9.6503\n",
      "\t\t -- train_counter: 13382, test_counter:399\n",
      "Sat Jan  1 23:55:09 2022\tEpoch: 099, Loss: 0.1300, Val: 5.5127, Test: 5.4375\n",
      "\t\t -- train_counter: 13525, test_counter:1154\n",
      "Sat Jan  1 23:55:30 2022\tEpoch: 100, Loss: 0.2276, Val: 6.2690, Test: 6.0229\n",
      "\t\t -- train_counter: 13067, test_counter:1086\n",
      "Sat Jan  1 23:55:49 2022\tEpoch: 101, Loss: 0.1844, Val: 5.9617, Test: 5.7775\n",
      "\t\t -- train_counter: 13113, test_counter:1127\n",
      "Sat Jan  1 23:56:09 2022\tEpoch: 102, Loss: 0.1760, Val: 6.2451, Test: 6.1085\n",
      "\t\t -- train_counter: 13197, test_counter:895\n",
      "Sat Jan  1 23:56:28 2022\tEpoch: 103, Loss: 0.2461, Val: 6.8172, Test: 6.5651\n",
      "\t\t -- train_counter: 13021, test_counter:1089\n",
      "Sat Jan  1 23:56:48 2022\tEpoch: 104, Loss: 0.4479, Val: 18.1352, Test: 17.6986\n",
      "\t\t -- train_counter: 11888, test_counter:373\n",
      "Sat Jan  1 23:57:09 2022\tEpoch: 105, Loss: 0.2429, Val: 6.5377, Test: 6.4067\n",
      "\t\t -- train_counter: 12862, test_counter:877\n",
      "Sat Jan  1 23:57:28 2022\tEpoch: 106, Loss: 0.1973, Val: 7.3046, Test: 7.1104\n",
      "\t\t -- train_counter: 13101, test_counter:805\n",
      "Sat Jan  1 23:57:48 2022\tEpoch: 107, Loss: 0.1353, Val: 6.3182, Test: 6.1643\n",
      "\t\t -- train_counter: 13476, test_counter:1127\n",
      "Sat Jan  1 23:58:08 2022\tEpoch: 108, Loss: 0.1012, Val: 7.7348, Test: 7.4598\n",
      "\t\t -- train_counter: 13653, test_counter:682\n",
      "Sat Jan  1 23:58:27 2022\tEpoch: 109, Loss: 0.0842, Val: 6.0317, Test: 5.7739\n",
      "\t\t -- train_counter: 13742, test_counter:1159\n",
      "Sat Jan  1 23:58:48 2022\tEpoch: 110, Loss: 0.0720, Val: 11.6643, Test: 11.4891\n",
      "\t\t -- train_counter: 13769, test_counter:713\n",
      "Sat Jan  1 23:59:09 2022\tEpoch: 111, Loss: 0.0542, Val: 6.2458, Test: 6.0222\n",
      "\t\t -- train_counter: 13882, test_counter:1062\n",
      "Sat Jan  1 23:59:31 2022\tEpoch: 112, Loss: 0.0443, Val: 6.4305, Test: 6.2447\n",
      "\t\t -- train_counter: 13925, test_counter:1049\n",
      "Sat Jan  1 23:59:53 2022\tEpoch: 113, Loss: 0.0899, Val: 7.4497, Test: 7.2600\n",
      "\t\t -- train_counter: 13741, test_counter:984\n",
      "Sun Jan  2 00:00:15 2022\tEpoch: 114, Loss: 0.3120, Val: 8.6213, Test: 8.4486\n",
      "\t\t -- train_counter: 12523, test_counter:902\n",
      "Sun Jan  2 00:00:37 2022\tEpoch: 115, Loss: 0.2151, Val: 6.1051, Test: 5.9084\n",
      "\t\t -- train_counter: 12954, test_counter:1155\n",
      "Sun Jan  2 00:00:58 2022\tEpoch: 116, Loss: 0.1372, Val: 8.7507, Test: 8.5496\n",
      "\t\t -- train_counter: 13405, test_counter:666\n",
      "Sun Jan  2 00:01:21 2022\tEpoch: 117, Loss: 0.1580, Val: 19.8979, Test: 19.4263\n",
      "\t\t -- train_counter: 13270, test_counter:430\n",
      "Sun Jan  2 00:01:43 2022\tEpoch: 118, Loss: 0.1380, Val: 12.0912, Test: 12.0238\n",
      "\t\t -- train_counter: 13399, test_counter:512\n",
      "Sun Jan  2 00:02:06 2022\tEpoch: 119, Loss: 0.0962, Val: 14.1233, Test: 13.7050\n",
      "\t\t -- train_counter: 13634, test_counter:609\n",
      "Sun Jan  2 00:02:28 2022\tEpoch: 120, Loss: 0.0986, Val: 11.0524, Test: 10.7931\n",
      "\t\t -- train_counter: 13638, test_counter:661\n",
      "Sun Jan  2 00:02:49 2022\tEpoch: 121, Loss: 0.1451, Val: 6.4349, Test: 6.1468\n",
      "\t\t -- train_counter: 13318, test_counter:1088\n",
      "Sun Jan  2 00:03:12 2022\tEpoch: 122, Loss: 0.1451, Val: 6.6799, Test: 6.4729\n",
      "\t\t -- train_counter: 13357, test_counter:1122\n",
      "Sun Jan  2 00:03:33 2022\tEpoch: 123, Loss: 0.6086, Val: 7.9621, Test: 7.6226\n",
      "\t\t -- train_counter: 11484, test_counter:945\n",
      "Sun Jan  2 00:03:55 2022\tEpoch: 124, Loss: 0.4297, Val: 12.4657, Test: 11.8982\n",
      "\t\t -- train_counter: 12009, test_counter:672\n",
      "Sun Jan  2 00:04:17 2022\tEpoch: 125, Loss: 0.2964, Val: 14.3684, Test: 14.3564\n",
      "\t\t -- train_counter: 12670, test_counter:406\n",
      "Sun Jan  2 00:04:39 2022\tEpoch: 126, Loss: 0.1653, Val: 6.5168, Test: 6.1678\n",
      "\t\t -- train_counter: 13269, test_counter:1103\n",
      "Sun Jan  2 00:05:01 2022\tEpoch: 127, Loss: 0.0944, Val: 11.0753, Test: 10.9172\n",
      "\t\t -- train_counter: 13657, test_counter:411\n",
      "Sun Jan  2 00:05:23 2022\tEpoch: 128, Loss: 0.0630, Val: 8.5671, Test: 8.3982\n",
      "\t\t -- train_counter: 13837, test_counter:662\n",
      "Sun Jan  2 00:05:44 2022\tEpoch: 129, Loss: 0.0395, Val: 7.1110, Test: 6.9667\n",
      "\t\t -- train_counter: 13937, test_counter:851\n",
      "Sun Jan  2 00:06:06 2022\tEpoch: 130, Loss: 0.0264, Val: 6.2211, Test: 6.0131\n",
      "\t\t -- train_counter: 13987, test_counter:1167\n",
      "Sun Jan  2 00:06:30 2022\tEpoch: 131, Loss: 0.0196, Val: 6.3036, Test: 6.0419\n",
      "\t\t -- train_counter: 13988, test_counter:1145\n",
      "Sun Jan  2 00:06:51 2022\tEpoch: 132, Loss: 0.0163, Val: 6.2511, Test: 6.0122\n",
      "\t\t -- train_counter: 13995, test_counter:1149\n",
      "Sun Jan  2 00:07:13 2022\tEpoch: 133, Loss: 0.0133, Val: 7.0930, Test: 6.8674\n",
      "\t\t -- train_counter: 13995, test_counter:946\n",
      "Sun Jan  2 00:07:36 2022\tEpoch: 134, Loss: 0.0126, Val: 7.6524, Test: 7.4195\n",
      "\t\t -- train_counter: 13995, test_counter:865\n",
      "Sun Jan  2 00:07:58 2022\tEpoch: 135, Loss: 0.0123, Val: 6.5062, Test: 6.2303\n",
      "\t\t -- train_counter: 13995, test_counter:1140\n",
      "Sun Jan  2 00:08:20 2022\tEpoch: 136, Loss: 0.0108, Val: 6.5270, Test: 6.2584\n",
      "\t\t -- train_counter: 13995, test_counter:1182\n",
      "Sun Jan  2 00:08:41 2022\tEpoch: 137, Loss: 0.0107, Val: 6.5379, Test: 6.2970\n",
      "\t\t -- train_counter: 13996, test_counter:1121\n",
      "Sun Jan  2 00:09:03 2022\tEpoch: 138, Loss: 0.0093, Val: 6.8501, Test: 6.6114\n",
      "\t\t -- train_counter: 13996, test_counter:1055\n",
      "Sun Jan  2 00:09:25 2022\tEpoch: 139, Loss: 0.4492, Val: 52.2391, Test: 51.8586\n",
      "\t\t -- train_counter: 12503, test_counter:325\n",
      "Sun Jan  2 00:09:47 2022\tEpoch: 140, Loss: 0.8866, Val: 40.5216, Test: 40.0902\n",
      "\t\t -- train_counter: 10344, test_counter:345\n",
      "Sun Jan  2 00:10:10 2022\tEpoch: 141, Loss: 0.5990, Val: 18.4171, Test: 18.5861\n",
      "\t\t -- train_counter: 11110, test_counter:380\n",
      "Sun Jan  2 00:10:34 2022\tEpoch: 142, Loss: 0.3570, Val: 6.9345, Test: 6.6765\n",
      "\t\t -- train_counter: 12225, test_counter:1025\n",
      "Sun Jan  2 00:10:57 2022\tEpoch: 143, Loss: 0.2540, Val: 13.5951, Test: 13.4268\n",
      "\t\t -- train_counter: 12827, test_counter:178\n",
      "Sun Jan  2 00:11:19 2022\tEpoch: 144, Loss: 0.1681, Val: 11.9986, Test: 11.9413\n",
      "\t\t -- train_counter: 13245, test_counter:257\n",
      "Sun Jan  2 00:11:41 2022\tEpoch: 145, Loss: 0.1086, Val: 8.0833, Test: 7.8769\n",
      "\t\t -- train_counter: 13590, test_counter:554\n",
      "Sun Jan  2 00:12:05 2022\tEpoch: 146, Loss: 0.0689, Val: 11.6467, Test: 11.5426\n",
      "\t\t -- train_counter: 13819, test_counter:292\n",
      "Sun Jan  2 00:12:28 2022\tEpoch: 147, Loss: 0.0568, Val: 11.3636, Test: 11.2194\n",
      "\t\t -- train_counter: 13862, test_counter:393\n",
      "Sun Jan  2 00:12:52 2022\tEpoch: 148, Loss: 0.0419, Val: 6.0332, Test: 5.8758\n",
      "\t\t -- train_counter: 13925, test_counter:1035\n",
      "Sun Jan  2 00:13:14 2022\tEpoch: 149, Loss: 0.0294, Val: 6.0223, Test: 5.7794\n",
      "\t\t -- train_counter: 13958, test_counter:1132\n",
      "Sun Jan  2 00:13:37 2022\tEpoch: 150, Loss: 0.0219, Val: 6.1521, Test: 5.9205\n",
      "\t\t -- train_counter: 13984, test_counter:1084\n",
      "Sun Jan  2 00:13:59 2022\tEpoch: 151, Loss: 0.0175, Val: 6.3604, Test: 6.1125\n",
      "\t\t -- train_counter: 13995, test_counter:1058\n",
      "Sun Jan  2 00:14:23 2022\tEpoch: 152, Loss: 0.0138, Val: 6.2319, Test: 5.9646\n",
      "\t\t -- train_counter: 13996, test_counter:1105\n",
      "Sun Jan  2 00:14:46 2022\tEpoch: 153, Loss: 0.0120, Val: 6.2207, Test: 5.9482\n",
      "\t\t -- train_counter: 13996, test_counter:1179\n",
      "Sun Jan  2 00:15:09 2022\tEpoch: 154, Loss: 0.0129, Val: 6.3626, Test: 6.1085\n",
      "\t\t -- train_counter: 13996, test_counter:1111\n",
      "Sun Jan  2 00:15:32 2022\tEpoch: 155, Loss: 0.0101, Val: 6.2912, Test: 6.0461\n",
      "\t\t -- train_counter: 13997, test_counter:1132\n",
      "Sun Jan  2 00:15:55 2022\tEpoch: 156, Loss: 0.0089, Val: 6.3204, Test: 6.0633\n",
      "\t\t -- train_counter: 13998, test_counter:1173\n",
      "Sun Jan  2 00:16:17 2022\tEpoch: 157, Loss: 0.0085, Val: 6.5394, Test: 6.2861\n",
      "\t\t -- train_counter: 13998, test_counter:1088\n",
      "Sun Jan  2 00:16:39 2022\tEpoch: 158, Loss: 0.0085, Val: 6.3783, Test: 6.1360\n",
      "\t\t -- train_counter: 13998, test_counter:1130\n",
      "Sun Jan  2 00:17:01 2022\tEpoch: 159, Loss: 0.0086, Val: 6.8113, Test: 6.5579\n",
      "\t\t -- train_counter: 13998, test_counter:1046\n",
      "Sun Jan  2 00:17:23 2022\tEpoch: 160, Loss: 0.0340, Val: 12.5104, Test: 12.1523\n",
      "\t\t -- train_counter: 13873, test_counter:664\n",
      "Sun Jan  2 00:17:45 2022\tEpoch: 161, Loss: 0.0325, Val: 6.9129, Test: 6.6588\n",
      "\t\t -- train_counter: 13921, test_counter:1044\n",
      "Sun Jan  2 00:18:07 2022\tEpoch: 162, Loss: 0.0303, Val: 6.3909, Test: 6.2534\n",
      "\t\t -- train_counter: 13946, test_counter:1179\n",
      "Sun Jan  2 00:18:30 2022\tEpoch: 163, Loss: 0.0209, Val: 6.6051, Test: 6.4050\n",
      "\t\t -- train_counter: 13975, test_counter:1124\n",
      "Sun Jan  2 00:18:51 2022\tEpoch: 164, Loss: 0.0149, Val: 6.5504, Test: 6.3411\n",
      "\t\t -- train_counter: 13989, test_counter:1158\n",
      "Sun Jan  2 00:19:13 2022\tEpoch: 165, Loss: 0.0108, Val: 7.3156, Test: 7.0681\n",
      "\t\t -- train_counter: 13998, test_counter:989\n",
      "Sun Jan  2 00:19:35 2022\tEpoch: 166, Loss: 0.0094, Val: 7.5546, Test: 7.2890\n",
      "\t\t -- train_counter: 13994, test_counter:965\n",
      "Sun Jan  2 00:19:58 2022\tEpoch: 167, Loss: 0.0072, Val: 6.6272, Test: 6.3995\n",
      "\t\t -- train_counter: 13998, test_counter:1150\n",
      "Sun Jan  2 00:20:20 2022\tEpoch: 168, Loss: 0.0061, Val: 7.0391, Test: 6.7828\n",
      "\t\t -- train_counter: 14000, test_counter:1054\n",
      "Sun Jan  2 00:20:41 2022\tEpoch: 169, Loss: 0.0061, Val: 6.6951, Test: 6.4524\n",
      "\t\t -- train_counter: 14000, test_counter:1138\n",
      "Sun Jan  2 00:21:04 2022\tEpoch: 170, Loss: 0.0053, Val: 6.8903, Test: 6.6400\n",
      "\t\t -- train_counter: 14000, test_counter:1092\n",
      "Sun Jan  2 00:21:27 2022\tEpoch: 171, Loss: 0.0053, Val: 6.6219, Test: 6.3856\n",
      "\t\t -- train_counter: 14000, test_counter:1169\n",
      "Sun Jan  2 00:21:48 2022\tEpoch: 172, Loss: 0.0052, Val: 6.7359, Test: 6.4900\n",
      "\t\t -- train_counter: 14000, test_counter:1197\n",
      "Sun Jan  2 00:22:11 2022\tEpoch: 173, Loss: 0.0053, Val: 6.7172, Test: 6.4812\n",
      "\t\t -- train_counter: 14000, test_counter:1126\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for epoch in range(1, 1001):\n",
    "    loss, train_counter = train(train_loader)\n",
    "    test_mae, test_counter = test(test_loader)\n",
    "    val_mae, _ = test(val_loader)\n",
    "    \n",
    "    # scheduler.step(loss)\n",
    "    \n",
    "    writer.add_scalar('Loss/train', loss, epoch)\n",
    "    writer.add_scalar('Loss/test', test_mae, epoch)\n",
    "    writer.add_scalar('Loss/val', val_mae, epoch)\n",
    "    writer.add_scalar('Counter/train', train_counter/len(train_loader.dataset), epoch)\n",
    "    writer.add_scalar('Counter/test', test_counter/len(test_loader.dataset), epoch)\n",
    "    \n",
    "    print(f'{time.ctime()}\\t'\n",
    "          f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_mae:.4f}, '\n",
    "          f'Test: {test_mae:.4f}')\n",
    "    \n",
    "    print(f'\\t\\t -- train_counter: {train_counter}, test_counter:{test_counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb52095-743c-4a75-947d-7e728e87c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b771e75-9824-4e0b-9d67-b98b87acdb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tld0 = list(train_loader)[0].to(device)\n",
    "tld1 = list(test_loader)[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb990c5f-3804-447f-9ec4-ff38f99732e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res0 = model(tld0.x, tld0.edge_index, tld0.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c526c-1fa0-4498-9f4d-5510a9d5c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "res0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f709262e-4b24-4720-98d5-d3d934d80962",
   "metadata": {},
   "outputs": [],
   "source": [
    "res0.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f9b9c-5456-4492-90ed-cbe9c261d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tld0.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e86c48-0a60-48b8-ac35-8776deadb25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(res0, tld0.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b7691-9bec-4d6f-ba36-07213d45cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1Loss()(res0.argmax(axis = 1).to(torch.float), tld0.y.to(torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a3380-01eb-4ff9-9485-ca018689bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(res0.argmax(axis = 1) - tld0.y).abs().sum().item()/len(tld0.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f9c83-9877-4df9-a4d4-235e1facaabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = model(tld1.x, tld1.edge_index, tld1.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092b0b1f-3a5f-427e-abb4-3ef7723ef7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488421d-cdc8-4ea1-9617-b0fbedeab631",
   "metadata": {},
   "outputs": [],
   "source": [
    "tld1.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df503c-618d-40c8-a040-a3cc36f23817",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(res1, tld1.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e359cd-f57b-4c36-9854-e893f651c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1Loss()(res1.argmax(axis = 1).to(torch.float), tld1.y.to(torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee4bdc-004c-4a80-ab9b-322a83e784f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = [d.y.item() for d in train_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6323a-9b77-4f60-b075-c8dbfdf37c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8702e1-17db-4b05-a5d9-9a02d86bd726",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = [d.y.item() for d in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb9e24-7048-404c-9cff-01b3025ce878",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432e542f-d058-4e5c-9ccc-f2ce3828072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique([d.y.item() for d in val_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ec70e5-62dc-4e3f-b3fa-aa666e6b3089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep AI",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
