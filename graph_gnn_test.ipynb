{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25aa328c-fbde-4c86-837d-b109eac25828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ModuleList, Embedding\n",
    "from torch.nn import Sequential, ReLU, Linear\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, PNAConv, BatchNorm, global_add_pool\n",
    "\n",
    "from gene_graph_dataset import GeneGraphDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1afe07e-f45d-4435-9efd-f7be458f83ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p, test_p = 0.7, 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c8f1633-bee2-464b-93e2-6fea1a471948",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GeneGraphDataset('dataset', 100, 5)\n",
    "data_size = len(dataset)\n",
    "train_size, test_size = (int)(data_size * train_p), (int)(data_size * test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a95dcce3-4aaf-4d68-b153-a9581899a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle()\n",
    "train_dataset = dataset[:train_size]\n",
    "test_dataset = dataset[train_size:(train_size + test_size)]\n",
    "val_dataset = dataset[(train_size + test_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63f6283c-d51a-4747-8f1b-f437ca523abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 100, 50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(train_dataset), len(test_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddbf1dce-6f12-4fdd-be53-37271e03d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89c85b3a-1bdf-49db-8494-d04d29ba39eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(train_loader), len(test_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "684cbd70-ea87-4b3d-b84a-54ef8dfb8c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = torch.zeros(5, dtype=torch.long)\n",
    "for data in train_dataset:\n",
    "    d = degree(data.edge_index[1].type(torch.int64), \n",
    "               num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ccffcdc-97e6-4d94-91a1-11fe35ec82b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.node_emb = Embedding(21, 75)\n",
    "        self.edge_emb = Embedding(4, 25)\n",
    "\n",
    "        aggregators = ['mean', 'min', 'max', 'std']\n",
    "        scalers = ['identity', 'amplification', 'attenuation']\n",
    "\n",
    "        self.convs = ModuleList()\n",
    "        self.batch_norms = ModuleList()\n",
    "        for _ in range(4):\n",
    "            conv = PNAConv(in_channels=75, out_channels=75,\n",
    "                           aggregators=aggregators, scalers=scalers, deg=deg,\n",
    "                           edge_dim=50, towers=5, pre_layers=1, post_layers=1,\n",
    "                           divide_input=False)\n",
    "            # conv = GCNConv(in_channels=75, out_channels=75)\n",
    "            self.convs.append(conv)\n",
    "            self.batch_norms.append(BatchNorm(75))\n",
    "            \n",
    "        self.pre_lin = Linear(150,75)\n",
    "\n",
    "        self.mlp = Sequential(Linear(75, 50), ReLU(), Linear(50, 25), ReLU(),\n",
    "                              Linear(25, 1))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        \n",
    "        x = torch.reshape(self.node_emb(x.squeeze()), (-1, 150))\n",
    "        x = self.pre_lin(x)\n",
    "        \n",
    "        edge_attr = torch.reshape(self.edge_emb(edge_attr), (-1,50))\n",
    "\n",
    "        for conv, batch_norm in zip(self.convs, self.batch_norms):\n",
    "            x = F.relu(batch_norm(conv(x, edge_index, edge_attr)))\n",
    "            # x = F.relu(batch_norm(conv(x, edge_index)))\n",
    "        x = global_add_pool(x, batch)        \n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6c79a8d-bb5d-4875-80ff-6c22ece243e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20,\n",
    "                              min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e364bfa2-7153-493c-9d74-ba7b1ae117ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        loss = (out.squeeze() - data.y).abs().mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce1b9829-772b-4e9b-a75e-1e9904d80afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_error = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        total_error += (out.squeeze() - data.y).abs().sum().item()\n",
    "    return total_error / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e23bb307-e484-493c-a60e-69911d93d554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 22 17:11:17 2021\tEpoch: 01, Loss: 6.1938, Val: 3.1765, Test: 2.9964\n",
      "Mon Nov 22 17:11:19 2021\tEpoch: 02, Loss: 1.7099, Val: 2.7765, Test: 2.5961\n",
      "Mon Nov 22 17:11:22 2021\tEpoch: 03, Loss: 1.6319, Val: 2.2906, Test: 2.1094\n",
      "Mon Nov 22 17:11:24 2021\tEpoch: 04, Loss: 1.2005, Val: 2.0680, Test: 1.9179\n",
      "Mon Nov 22 17:11:27 2021\tEpoch: 05, Loss: 1.1400, Val: 1.9023, Test: 1.7593\n",
      "Mon Nov 22 17:11:29 2021\tEpoch: 06, Loss: 1.1348, Val: 1.7413, Test: 1.5886\n",
      "Mon Nov 22 17:11:32 2021\tEpoch: 07, Loss: 1.1368, Val: 1.8401, Test: 1.6766\n",
      "Mon Nov 22 17:11:34 2021\tEpoch: 08, Loss: 1.0848, Val: 1.7011, Test: 1.5493\n",
      "Mon Nov 22 17:11:36 2021\tEpoch: 09, Loss: 1.0623, Val: 1.8407, Test: 1.6330\n",
      "Mon Nov 22 17:11:39 2021\tEpoch: 10, Loss: 1.0582, Val: 2.7903, Test: 2.6462\n",
      "Mon Nov 22 17:11:41 2021\tEpoch: 11, Loss: 1.0253, Val: 3.1026, Test: 2.9518\n",
      "Mon Nov 22 17:11:44 2021\tEpoch: 12, Loss: 1.0080, Val: 3.4607, Test: 3.3015\n",
      "Mon Nov 22 17:11:46 2021\tEpoch: 13, Loss: 1.2310, Val: 3.7483, Test: 3.5069\n",
      "Mon Nov 22 17:11:49 2021\tEpoch: 14, Loss: 1.0978, Val: 1.8722, Test: 2.0497\n",
      "Mon Nov 22 17:11:51 2021\tEpoch: 15, Loss: 1.0658, Val: 3.7896, Test: 3.8439\n",
      "Mon Nov 22 17:11:53 2021\tEpoch: 16, Loss: 1.0109, Val: 1.6192, Test: 1.6703\n",
      "Mon Nov 22 17:11:56 2021\tEpoch: 17, Loss: 0.9698, Val: 1.3255, Test: 1.2694\n",
      "Mon Nov 22 17:11:58 2021\tEpoch: 18, Loss: 0.9353, Val: 1.2597, Test: 1.1235\n",
      "Mon Nov 22 17:12:01 2021\tEpoch: 19, Loss: 0.9547, Val: 1.3969, Test: 1.3371\n",
      "Mon Nov 22 17:12:03 2021\tEpoch: 20, Loss: 1.0391, Val: 1.0340, Test: 1.0598\n",
      "Mon Nov 22 17:12:06 2021\tEpoch: 21, Loss: 1.0152, Val: 1.5721, Test: 1.5816\n",
      "Mon Nov 22 17:12:08 2021\tEpoch: 22, Loss: 0.9644, Val: 2.2789, Test: 2.3358\n",
      "Mon Nov 22 17:12:10 2021\tEpoch: 23, Loss: 0.9604, Val: 1.5391, Test: 1.6330\n",
      "Mon Nov 22 17:12:13 2021\tEpoch: 24, Loss: 0.8617, Val: 2.3461, Test: 2.4421\n",
      "Mon Nov 22 17:12:15 2021\tEpoch: 25, Loss: 0.8623, Val: 1.0355, Test: 1.1075\n",
      "Mon Nov 22 17:12:18 2021\tEpoch: 26, Loss: 0.8838, Val: 1.1393, Test: 1.1802\n",
      "Mon Nov 22 17:12:20 2021\tEpoch: 27, Loss: 0.8397, Val: 0.9670, Test: 0.9410\n",
      "Mon Nov 22 17:12:23 2021\tEpoch: 28, Loss: 0.8295, Val: 1.1742, Test: 1.2075\n",
      "Mon Nov 22 17:12:25 2021\tEpoch: 29, Loss: 0.8778, Val: 1.2154, Test: 1.0779\n",
      "Mon Nov 22 17:12:28 2021\tEpoch: 30, Loss: 0.8221, Val: 0.8904, Test: 0.9496\n",
      "Mon Nov 22 17:12:30 2021\tEpoch: 31, Loss: 0.8556, Val: 1.6200, Test: 1.6506\n",
      "Mon Nov 22 17:12:32 2021\tEpoch: 32, Loss: 0.8034, Val: 1.3818, Test: 1.4858\n",
      "Mon Nov 22 17:12:35 2021\tEpoch: 33, Loss: 0.8187, Val: 1.0660, Test: 1.2145\n",
      "Mon Nov 22 17:12:37 2021\tEpoch: 34, Loss: 0.7934, Val: 0.9952, Test: 0.9981\n",
      "Mon Nov 22 17:12:40 2021\tEpoch: 35, Loss: 0.8165, Val: 1.6656, Test: 1.7503\n",
      "Mon Nov 22 17:12:42 2021\tEpoch: 36, Loss: 0.8447, Val: 1.4522, Test: 1.3367\n",
      "Mon Nov 22 17:12:45 2021\tEpoch: 37, Loss: 0.7639, Val: 1.0635, Test: 1.0577\n",
      "Mon Nov 22 17:12:47 2021\tEpoch: 38, Loss: 0.7680, Val: 1.4825, Test: 1.4097\n",
      "Mon Nov 22 17:12:49 2021\tEpoch: 39, Loss: 0.7457, Val: 1.3255, Test: 1.2457\n",
      "Mon Nov 22 17:12:52 2021\tEpoch: 40, Loss: 0.7260, Val: 1.2053, Test: 1.1865\n",
      "Mon Nov 22 17:12:54 2021\tEpoch: 41, Loss: 0.7213, Val: 1.2723, Test: 1.2104\n",
      "Mon Nov 22 17:12:57 2021\tEpoch: 42, Loss: 0.7165, Val: 1.1769, Test: 1.1462\n",
      "Mon Nov 22 17:12:59 2021\tEpoch: 43, Loss: 0.8428, Val: 0.8321, Test: 0.9309\n",
      "Mon Nov 22 17:13:02 2021\tEpoch: 44, Loss: 0.8178, Val: 1.3004, Test: 1.1794\n",
      "Mon Nov 22 17:13:04 2021\tEpoch: 45, Loss: 0.7650, Val: 1.3958, Test: 1.3108\n",
      "Mon Nov 22 17:13:07 2021\tEpoch: 46, Loss: 0.7388, Val: 0.8778, Test: 0.8761\n",
      "Mon Nov 22 17:13:09 2021\tEpoch: 47, Loss: 0.7738, Val: 0.9959, Test: 0.8614\n",
      "Mon Nov 22 17:13:11 2021\tEpoch: 48, Loss: 0.7384, Val: 0.8850, Test: 1.0330\n",
      "Mon Nov 22 17:13:14 2021\tEpoch: 49, Loss: 0.6743, Val: 1.2405, Test: 1.0758\n",
      "Mon Nov 22 17:13:16 2021\tEpoch: 50, Loss: 0.6793, Val: 0.9815, Test: 0.9738\n",
      "Mon Nov 22 17:13:19 2021\tEpoch: 51, Loss: 0.7032, Val: 0.8773, Test: 0.8407\n",
      "Mon Nov 22 17:13:21 2021\tEpoch: 52, Loss: 0.6487, Val: 0.8541, Test: 0.9720\n",
      "Mon Nov 22 17:13:24 2021\tEpoch: 53, Loss: 0.6302, Val: 1.3011, Test: 1.3927\n",
      "Mon Nov 22 17:13:26 2021\tEpoch: 54, Loss: 0.7317, Val: 0.7810, Test: 0.8300\n",
      "Mon Nov 22 17:13:29 2021\tEpoch: 55, Loss: 0.6371, Val: 1.8176, Test: 1.6873\n",
      "Mon Nov 22 17:13:31 2021\tEpoch: 56, Loss: 0.7215, Val: 0.9484, Test: 1.0606\n",
      "Mon Nov 22 17:13:33 2021\tEpoch: 57, Loss: 0.7060, Val: 1.0971, Test: 1.2004\n",
      "Mon Nov 22 17:13:36 2021\tEpoch: 58, Loss: 0.6721, Val: 1.6019, Test: 1.7796\n",
      "Mon Nov 22 17:13:38 2021\tEpoch: 59, Loss: 0.7535, Val: 0.9552, Test: 0.8836\n",
      "Mon Nov 22 17:13:41 2021\tEpoch: 60, Loss: 0.6636, Val: 0.8716, Test: 0.8903\n",
      "Mon Nov 22 17:13:43 2021\tEpoch: 61, Loss: 0.7182, Val: 1.2130, Test: 1.0887\n",
      "Mon Nov 22 17:13:46 2021\tEpoch: 62, Loss: 0.6901, Val: 1.1278, Test: 0.9853\n",
      "Mon Nov 22 17:13:48 2021\tEpoch: 63, Loss: 0.7310, Val: 0.7554, Test: 0.8819\n",
      "Mon Nov 22 17:13:50 2021\tEpoch: 64, Loss: 0.7438, Val: 0.9042, Test: 0.8729\n",
      "Mon Nov 22 17:13:53 2021\tEpoch: 65, Loss: 0.6577, Val: 1.1265, Test: 1.0110\n",
      "Mon Nov 22 17:13:55 2021\tEpoch: 66, Loss: 0.6437, Val: 0.8503, Test: 0.9366\n",
      "Mon Nov 22 17:13:58 2021\tEpoch: 67, Loss: 0.6158, Val: 0.9777, Test: 0.9415\n",
      "Mon Nov 22 17:14:00 2021\tEpoch: 68, Loss: 0.8505, Val: 1.4913, Test: 1.6087\n",
      "Mon Nov 22 17:14:03 2021\tEpoch: 69, Loss: 0.8807, Val: 1.9583, Test: 2.1580\n",
      "Mon Nov 22 17:14:05 2021\tEpoch: 70, Loss: 0.8683, Val: 0.7391, Test: 0.8072\n",
      "Mon Nov 22 17:14:08 2021\tEpoch: 71, Loss: 0.7541, Val: 2.3242, Test: 2.4141\n",
      "Mon Nov 22 17:14:10 2021\tEpoch: 72, Loss: 0.7263, Val: 0.8942, Test: 1.0715\n",
      "Mon Nov 22 17:14:12 2021\tEpoch: 73, Loss: 0.6416, Val: 1.6450, Test: 1.7528\n",
      "Mon Nov 22 17:14:15 2021\tEpoch: 74, Loss: 0.6569, Val: 0.7768, Test: 0.8561\n",
      "Mon Nov 22 17:14:17 2021\tEpoch: 75, Loss: 0.6165, Val: 0.7341, Test: 0.9215\n",
      "Mon Nov 22 17:14:20 2021\tEpoch: 76, Loss: 0.6818, Val: 1.1975, Test: 1.1212\n",
      "Mon Nov 22 17:14:22 2021\tEpoch: 77, Loss: 0.7032, Val: 0.8754, Test: 0.8790\n",
      "Mon Nov 22 17:14:25 2021\tEpoch: 78, Loss: 0.5990, Val: 0.9713, Test: 0.8966\n",
      "Mon Nov 22 17:14:27 2021\tEpoch: 79, Loss: 0.6417, Val: 0.8045, Test: 0.8287\n",
      "Mon Nov 22 17:14:30 2021\tEpoch: 80, Loss: 0.6103, Val: 2.3681, Test: 2.5371\n",
      "Mon Nov 22 17:14:32 2021\tEpoch: 81, Loss: 0.6665, Val: 0.8336, Test: 0.8006\n",
      "Mon Nov 22 17:14:34 2021\tEpoch: 82, Loss: 0.7184, Val: 1.1661, Test: 1.2878\n",
      "Mon Nov 22 17:14:37 2021\tEpoch: 83, Loss: 0.5873, Val: 0.8378, Test: 0.7868\n",
      "Mon Nov 22 17:14:39 2021\tEpoch: 84, Loss: 0.5685, Val: 0.6793, Test: 0.7753\n",
      "Mon Nov 22 17:14:42 2021\tEpoch: 85, Loss: 0.5568, Val: 0.6243, Test: 0.7367\n",
      "Mon Nov 22 17:14:44 2021\tEpoch: 86, Loss: 0.5807, Val: 1.2402, Test: 1.1614\n",
      "Mon Nov 22 17:14:47 2021\tEpoch: 87, Loss: 0.5779, Val: 0.6550, Test: 0.7780\n",
      "Mon Nov 22 17:14:49 2021\tEpoch: 88, Loss: 0.5375, Val: 1.3641, Test: 1.2079\n",
      "Mon Nov 22 17:14:52 2021\tEpoch: 89, Loss: 0.5425, Val: 0.6793, Test: 0.7918\n",
      "Mon Nov 22 17:14:54 2021\tEpoch: 90, Loss: 0.5641, Val: 0.6023, Test: 0.7109\n",
      "Mon Nov 22 17:14:56 2021\tEpoch: 91, Loss: 0.5280, Val: 0.7674, Test: 0.7564\n",
      "Mon Nov 22 17:14:59 2021\tEpoch: 92, Loss: 0.6120, Val: 0.6814, Test: 0.8391\n",
      "Mon Nov 22 17:15:01 2021\tEpoch: 93, Loss: 0.5317, Val: 0.8210, Test: 0.8025\n",
      "Mon Nov 22 17:15:04 2021\tEpoch: 94, Loss: 0.4997, Val: 0.7395, Test: 0.7962\n",
      "Mon Nov 22 17:15:06 2021\tEpoch: 95, Loss: 0.5509, Val: 1.0637, Test: 1.2629\n",
      "Mon Nov 22 17:15:09 2021\tEpoch: 96, Loss: 0.5205, Val: 0.5826, Test: 0.6909\n",
      "Mon Nov 22 17:15:11 2021\tEpoch: 97, Loss: 0.5209, Val: 0.6832, Test: 0.8937\n",
      "Mon Nov 22 17:15:14 2021\tEpoch: 98, Loss: 0.4909, Val: 0.6597, Test: 0.8237\n",
      "Mon Nov 22 17:15:16 2021\tEpoch: 99, Loss: 0.4763, Val: 0.6390, Test: 0.7794\n",
      "Mon Nov 22 17:15:18 2021\tEpoch: 100, Loss: 0.4912, Val: 1.1372, Test: 1.3463\n",
      "Mon Nov 22 17:15:21 2021\tEpoch: 101, Loss: 0.5808, Val: 0.8863, Test: 0.8516\n",
      "Mon Nov 22 17:15:23 2021\tEpoch: 102, Loss: 0.4702, Val: 1.2196, Test: 1.3523\n",
      "Mon Nov 22 17:15:26 2021\tEpoch: 103, Loss: 0.5918, Val: 0.6139, Test: 0.7373\n",
      "Mon Nov 22 17:15:28 2021\tEpoch: 104, Loss: 0.5648, Val: 0.7911, Test: 0.9522\n",
      "Mon Nov 22 17:15:31 2021\tEpoch: 105, Loss: 0.5495, Val: 0.6074, Test: 0.7154\n",
      "Mon Nov 22 17:15:33 2021\tEpoch: 106, Loss: 0.5340, Val: 0.7582, Test: 0.7468\n",
      "Mon Nov 22 17:15:36 2021\tEpoch: 107, Loss: 0.4863, Val: 0.8062, Test: 0.8457\n",
      "Mon Nov 22 17:15:38 2021\tEpoch: 108, Loss: 0.5010, Val: 0.7705, Test: 0.7438\n",
      "Mon Nov 22 17:15:40 2021\tEpoch: 109, Loss: 0.5469, Val: 1.3031, Test: 1.2091\n",
      "Mon Nov 22 17:15:43 2021\tEpoch: 110, Loss: 0.6443, Val: 0.8004, Test: 0.9935\n",
      "Mon Nov 22 17:15:45 2021\tEpoch: 111, Loss: 0.5661, Val: 1.6299, Test: 1.4926\n",
      "Mon Nov 22 17:15:48 2021\tEpoch: 112, Loss: 0.6446, Val: 1.3243, Test: 1.4562\n",
      "Mon Nov 22 17:15:50 2021\tEpoch: 113, Loss: 0.5586, Val: 0.8118, Test: 0.8043\n",
      "Mon Nov 22 17:15:53 2021\tEpoch: 114, Loss: 0.6631, Val: 1.0540, Test: 1.2376\n",
      "Mon Nov 22 17:15:55 2021\tEpoch: 115, Loss: 0.6053, Val: 1.2927, Test: 1.1189\n",
      "Mon Nov 22 17:15:57 2021\tEpoch: 116, Loss: 0.6242, Val: 0.8580, Test: 1.0473\n",
      "Mon Nov 22 17:16:00 2021\tEpoch: 117, Loss: 0.6464, Val: 0.7385, Test: 0.8176\n",
      "Mon Nov 22 17:16:02 2021\tEpoch: 118, Loss: 0.6154, Val: 0.7950, Test: 0.8539\n",
      "Mon Nov 22 17:16:05 2021\tEpoch: 119, Loss: 0.5333, Val: 0.8230, Test: 0.8455\n",
      "Mon Nov 22 17:16:07 2021\tEpoch: 120, Loss: 0.4640, Val: 0.9361, Test: 0.9139\n",
      "Mon Nov 22 17:16:10 2021\tEpoch: 121, Loss: 0.4552, Val: 0.9089, Test: 0.8857\n",
      "Mon Nov 22 17:16:12 2021\tEpoch: 122, Loss: 0.4479, Val: 0.8490, Test: 0.8293\n",
      "Mon Nov 22 17:16:15 2021\tEpoch: 123, Loss: 0.4452, Val: 0.8036, Test: 0.7843\n",
      "Mon Nov 22 17:16:17 2021\tEpoch: 124, Loss: 0.4243, Val: 0.5902, Test: 0.7066\n",
      "Mon Nov 22 17:16:19 2021\tEpoch: 125, Loss: 0.4486, Val: 0.6230, Test: 0.6807\n",
      "Mon Nov 22 17:16:22 2021\tEpoch: 126, Loss: 0.4173, Val: 0.7368, Test: 0.7398\n",
      "Mon Nov 22 17:16:24 2021\tEpoch: 127, Loss: 0.4284, Val: 0.5852, Test: 0.7049\n",
      "Mon Nov 22 17:16:27 2021\tEpoch: 128, Loss: 0.4009, Val: 0.8922, Test: 0.8506\n",
      "Mon Nov 22 17:16:29 2021\tEpoch: 129, Loss: 0.4262, Val: 0.6120, Test: 0.7081\n",
      "Mon Nov 22 17:16:32 2021\tEpoch: 130, Loss: 0.4095, Val: 0.6807, Test: 0.7014\n",
      "Mon Nov 22 17:16:34 2021\tEpoch: 131, Loss: 0.4019, Val: 0.6042, Test: 0.7155\n",
      "Mon Nov 22 17:16:37 2021\tEpoch: 132, Loss: 0.4061, Val: 0.5975, Test: 0.6556\n",
      "Mon Nov 22 17:16:39 2021\tEpoch: 133, Loss: 0.3912, Val: 0.6307, Test: 0.7252\n",
      "Mon Nov 22 17:16:41 2021\tEpoch: 134, Loss: 0.4130, Val: 0.6866, Test: 0.6947\n",
      "Mon Nov 22 17:16:44 2021\tEpoch: 135, Loss: 0.3788, Val: 0.6088, Test: 0.6845\n",
      "Mon Nov 22 17:16:46 2021\tEpoch: 136, Loss: 0.4020, Val: 0.8132, Test: 0.8201\n",
      "Mon Nov 22 17:16:49 2021\tEpoch: 137, Loss: 0.4055, Val: 0.5967, Test: 0.6930\n",
      "Mon Nov 22 17:16:51 2021\tEpoch: 138, Loss: 0.3627, Val: 0.8659, Test: 0.8521\n",
      "Mon Nov 22 17:16:54 2021\tEpoch: 139, Loss: 0.3936, Val: 0.5702, Test: 0.6634\n",
      "Mon Nov 22 17:16:56 2021\tEpoch: 140, Loss: 0.3978, Val: 0.5700, Test: 0.6774\n",
      "Mon Nov 22 17:16:59 2021\tEpoch: 141, Loss: 0.3745, Val: 0.6800, Test: 0.6985\n",
      "Mon Nov 22 17:17:01 2021\tEpoch: 142, Loss: 0.3875, Val: 0.7071, Test: 0.7221\n",
      "Mon Nov 22 17:17:03 2021\tEpoch: 143, Loss: 0.3741, Val: 0.5962, Test: 0.6968\n",
      "Mon Nov 22 17:17:06 2021\tEpoch: 144, Loss: 0.3680, Val: 0.6709, Test: 0.6885\n",
      "Mon Nov 22 17:17:08 2021\tEpoch: 145, Loss: 0.4008, Val: 0.5979, Test: 0.7092\n",
      "Mon Nov 22 17:17:11 2021\tEpoch: 146, Loss: 0.3851, Val: 0.5801, Test: 0.7012\n",
      "Mon Nov 22 17:17:13 2021\tEpoch: 147, Loss: 0.3606, Val: 0.6608, Test: 0.6651\n",
      "Mon Nov 22 17:17:16 2021\tEpoch: 148, Loss: 0.3576, Val: 0.5732, Test: 0.7312\n",
      "Mon Nov 22 17:17:18 2021\tEpoch: 149, Loss: 0.3705, Val: 0.5641, Test: 0.6664\n",
      "Mon Nov 22 17:17:20 2021\tEpoch: 150, Loss: 0.3738, Val: 0.7342, Test: 0.7388\n",
      "Mon Nov 22 17:17:23 2021\tEpoch: 151, Loss: 0.3663, Val: 0.5464, Test: 0.6660\n",
      "Mon Nov 22 17:17:25 2021\tEpoch: 152, Loss: 0.3574, Val: 0.5906, Test: 0.6599\n",
      "Mon Nov 22 17:17:28 2021\tEpoch: 153, Loss: 0.3580, Val: 0.5731, Test: 0.6449\n",
      "Mon Nov 22 17:17:30 2021\tEpoch: 154, Loss: 0.3896, Val: 0.5976, Test: 0.6496\n",
      "Mon Nov 22 17:17:33 2021\tEpoch: 155, Loss: 0.3461, Val: 0.5642, Test: 0.6563\n",
      "Mon Nov 22 17:17:35 2021\tEpoch: 156, Loss: 0.3952, Val: 0.5630, Test: 0.6518\n",
      "Mon Nov 22 17:17:38 2021\tEpoch: 157, Loss: 0.4244, Val: 0.5828, Test: 0.6539\n",
      "Mon Nov 22 17:17:40 2021\tEpoch: 158, Loss: 0.3756, Val: 0.5503, Test: 0.6338\n",
      "Mon Nov 22 17:17:42 2021\tEpoch: 159, Loss: 0.3510, Val: 0.5508, Test: 0.6781\n",
      "Mon Nov 22 17:17:45 2021\tEpoch: 160, Loss: 0.3556, Val: 0.7495, Test: 0.7408\n",
      "Mon Nov 22 17:17:47 2021\tEpoch: 161, Loss: 0.3537, Val: 0.5453, Test: 0.7161\n",
      "Mon Nov 22 17:17:50 2021\tEpoch: 162, Loss: 0.3862, Val: 0.6226, Test: 0.6554\n",
      "Mon Nov 22 17:17:52 2021\tEpoch: 163, Loss: 0.3514, Val: 0.5351, Test: 0.6855\n",
      "Mon Nov 22 17:17:55 2021\tEpoch: 164, Loss: 0.3469, Val: 0.5641, Test: 0.6911\n",
      "Mon Nov 22 17:17:57 2021\tEpoch: 165, Loss: 0.3744, Val: 0.7297, Test: 0.7180\n",
      "Mon Nov 22 17:18:00 2021\tEpoch: 166, Loss: 0.3854, Val: 0.6317, Test: 0.7121\n",
      "Mon Nov 22 17:18:02 2021\tEpoch: 167, Loss: 0.3519, Val: 0.6132, Test: 0.7320\n",
      "Mon Nov 22 17:18:04 2021\tEpoch: 168, Loss: 0.4194, Val: 0.7316, Test: 0.7224\n",
      "Mon Nov 22 17:18:07 2021\tEpoch: 169, Loss: 0.3718, Val: 0.5773, Test: 0.7011\n",
      "Mon Nov 22 17:18:09 2021\tEpoch: 170, Loss: 0.3676, Val: 0.5597, Test: 0.6779\n",
      "Mon Nov 22 17:18:12 2021\tEpoch: 171, Loss: 0.3457, Val: 0.7069, Test: 0.7083\n",
      "Mon Nov 22 17:18:14 2021\tEpoch: 172, Loss: 0.3710, Val: 0.5449, Test: 0.6633\n",
      "Mon Nov 22 17:18:17 2021\tEpoch: 173, Loss: 0.3517, Val: 0.6651, Test: 0.6853\n",
      "Mon Nov 22 17:18:19 2021\tEpoch: 174, Loss: 0.3748, Val: 0.5787, Test: 0.7056\n",
      "Mon Nov 22 17:18:22 2021\tEpoch: 175, Loss: 0.3691, Val: 0.5798, Test: 0.7056\n",
      "Mon Nov 22 17:18:24 2021\tEpoch: 176, Loss: 0.3725, Val: 0.5909, Test: 0.6608\n",
      "Mon Nov 22 17:18:26 2021\tEpoch: 177, Loss: 0.3387, Val: 0.5513, Test: 0.6788\n",
      "Mon Nov 22 17:18:29 2021\tEpoch: 178, Loss: 0.3959, Val: 0.5815, Test: 0.6834\n",
      "Mon Nov 22 17:18:31 2021\tEpoch: 179, Loss: 0.4108, Val: 0.5689, Test: 0.6575\n",
      "Mon Nov 22 17:18:34 2021\tEpoch: 180, Loss: 0.3969, Val: 0.6246, Test: 0.6740\n",
      "Mon Nov 22 17:18:36 2021\tEpoch: 181, Loss: 0.3466, Val: 0.6433, Test: 0.7833\n",
      "Mon Nov 22 17:18:39 2021\tEpoch: 182, Loss: 0.3892, Val: 0.5991, Test: 0.6549\n",
      "Mon Nov 22 17:18:41 2021\tEpoch: 183, Loss: 0.3847, Val: 0.6034, Test: 0.6386\n",
      "Mon Nov 22 17:18:44 2021\tEpoch: 184, Loss: 0.3375, Val: 0.5674, Test: 0.6590\n",
      "Mon Nov 22 17:18:46 2021\tEpoch: 185, Loss: 0.3453, Val: 0.5986, Test: 0.6618\n",
      "Mon Nov 22 17:18:48 2021\tEpoch: 186, Loss: 0.3274, Val: 0.7304, Test: 0.7369\n",
      "Mon Nov 22 17:18:51 2021\tEpoch: 187, Loss: 0.3344, Val: 0.5679, Test: 0.6451\n",
      "Mon Nov 22 17:18:53 2021\tEpoch: 188, Loss: 0.3243, Val: 0.5580, Test: 0.6742\n",
      "Mon Nov 22 17:18:56 2021\tEpoch: 189, Loss: 0.3685, Val: 0.5719, Test: 0.6294\n",
      "Mon Nov 22 17:18:58 2021\tEpoch: 190, Loss: 0.3345, Val: 0.5796, Test: 0.6332\n",
      "Mon Nov 22 17:19:01 2021\tEpoch: 191, Loss: 0.3688, Val: 0.5485, Test: 0.6674\n",
      "Mon Nov 22 17:19:03 2021\tEpoch: 192, Loss: 0.3189, Val: 0.5883, Test: 0.6435\n",
      "Mon Nov 22 17:19:06 2021\tEpoch: 193, Loss: 0.3141, Val: 0.5950, Test: 0.6447\n",
      "Mon Nov 22 17:19:08 2021\tEpoch: 194, Loss: 0.3730, Val: 0.5459, Test: 0.6513\n",
      "Mon Nov 22 17:19:10 2021\tEpoch: 195, Loss: 0.3404, Val: 0.5573, Test: 0.6452\n",
      "Mon Nov 22 17:19:13 2021\tEpoch: 196, Loss: 0.3260, Val: 0.5921, Test: 0.6472\n",
      "Mon Nov 22 17:19:15 2021\tEpoch: 197, Loss: 0.3219, Val: 0.5341, Test: 0.6488\n",
      "Mon Nov 22 17:19:18 2021\tEpoch: 198, Loss: 0.3155, Val: 0.5654, Test: 0.6392\n",
      "Mon Nov 22 17:19:20 2021\tEpoch: 199, Loss: 0.3621, Val: 0.6412, Test: 0.6653\n",
      "Mon Nov 22 17:19:23 2021\tEpoch: 200, Loss: 0.3501, Val: 0.5633, Test: 0.6838\n",
      "Mon Nov 22 17:19:25 2021\tEpoch: 201, Loss: 0.3491, Val: 0.5856, Test: 0.7156\n",
      "Mon Nov 22 17:19:27 2021\tEpoch: 202, Loss: 0.3203, Val: 0.6585, Test: 0.6777\n",
      "Mon Nov 22 17:19:30 2021\tEpoch: 203, Loss: 0.3222, Val: 0.5849, Test: 0.6388\n",
      "Mon Nov 22 17:19:32 2021\tEpoch: 204, Loss: 0.3095, Val: 0.5371, Test: 0.6767\n",
      "Mon Nov 22 17:19:35 2021\tEpoch: 205, Loss: 0.3660, Val: 0.5444, Test: 0.6342\n",
      "Mon Nov 22 17:19:37 2021\tEpoch: 206, Loss: 0.3348, Val: 0.5956, Test: 0.6325\n",
      "Mon Nov 22 17:19:40 2021\tEpoch: 207, Loss: 0.3195, Val: 0.5766, Test: 0.7205\n",
      "Mon Nov 22 17:19:42 2021\tEpoch: 208, Loss: 0.3252, Val: 0.5471, Test: 0.6409\n",
      "Mon Nov 22 17:19:45 2021\tEpoch: 209, Loss: 0.3198, Val: 0.5658, Test: 0.6201\n",
      "Mon Nov 22 17:19:47 2021\tEpoch: 210, Loss: 0.3283, Val: 0.5611, Test: 0.6994\n",
      "Mon Nov 22 17:19:49 2021\tEpoch: 211, Loss: 0.3642, Val: 0.5339, Test: 0.6385\n",
      "Mon Nov 22 17:19:52 2021\tEpoch: 212, Loss: 0.3475, Val: 0.5855, Test: 0.6316\n",
      "Mon Nov 22 17:19:54 2021\tEpoch: 213, Loss: 0.4315, Val: 0.6017, Test: 0.6478\n",
      "Mon Nov 22 17:19:57 2021\tEpoch: 214, Loss: 0.3184, Val: 0.5341, Test: 0.6555\n",
      "Mon Nov 22 17:19:59 2021\tEpoch: 215, Loss: 0.3317, Val: 0.5743, Test: 0.6420\n",
      "Mon Nov 22 17:20:02 2021\tEpoch: 216, Loss: 0.3180, Val: 0.5667, Test: 0.6337\n",
      "Mon Nov 22 17:20:04 2021\tEpoch: 217, Loss: 0.3067, Val: 0.5948, Test: 0.6464\n",
      "Mon Nov 22 17:20:07 2021\tEpoch: 218, Loss: 0.3134, Val: 0.5448, Test: 0.6355\n",
      "Mon Nov 22 17:20:09 2021\tEpoch: 219, Loss: 0.3476, Val: 0.5359, Test: 0.6325\n",
      "Mon Nov 22 17:20:11 2021\tEpoch: 220, Loss: 0.3015, Val: 0.6276, Test: 0.6725\n",
      "Mon Nov 22 17:20:14 2021\tEpoch: 221, Loss: 0.3381, Val: 0.5296, Test: 0.6367\n",
      "Mon Nov 22 17:20:16 2021\tEpoch: 222, Loss: 0.3038, Val: 0.5594, Test: 0.6677\n",
      "Mon Nov 22 17:20:19 2021\tEpoch: 223, Loss: 0.3179, Val: 0.5475, Test: 0.6271\n",
      "Mon Nov 22 17:20:21 2021\tEpoch: 224, Loss: 0.3106, Val: 0.5662, Test: 0.6958\n",
      "Mon Nov 22 17:20:24 2021\tEpoch: 225, Loss: 0.3783, Val: 0.5939, Test: 0.7286\n",
      "Mon Nov 22 17:20:26 2021\tEpoch: 226, Loss: 0.3462, Val: 0.5453, Test: 0.6300\n",
      "Mon Nov 22 17:20:28 2021\tEpoch: 227, Loss: 0.3466, Val: 0.5577, Test: 0.6526\n",
      "Mon Nov 22 17:20:31 2021\tEpoch: 228, Loss: 0.3179, Val: 0.5676, Test: 0.6561\n",
      "Mon Nov 22 17:20:33 2021\tEpoch: 229, Loss: 0.3147, Val: 0.5576, Test: 0.6461\n",
      "Mon Nov 22 17:20:36 2021\tEpoch: 230, Loss: 0.3683, Val: 0.5407, Test: 0.6475\n",
      "Mon Nov 22 17:20:38 2021\tEpoch: 231, Loss: 0.3348, Val: 0.5757, Test: 0.6386\n",
      "Mon Nov 22 17:20:41 2021\tEpoch: 232, Loss: 0.3254, Val: 0.5583, Test: 0.6387\n",
      "Mon Nov 22 17:20:43 2021\tEpoch: 233, Loss: 0.3281, Val: 0.5547, Test: 0.6596\n",
      "Mon Nov 22 17:20:46 2021\tEpoch: 234, Loss: 0.3117, Val: 0.6797, Test: 0.7000\n",
      "Mon Nov 22 17:20:48 2021\tEpoch: 235, Loss: 0.3240, Val: 0.5480, Test: 0.6420\n",
      "Mon Nov 22 17:20:50 2021\tEpoch: 236, Loss: 0.3030, Val: 0.6103, Test: 0.6679\n",
      "Mon Nov 22 17:20:53 2021\tEpoch: 237, Loss: 0.2882, Val: 0.5873, Test: 0.6462\n",
      "Mon Nov 22 17:20:55 2021\tEpoch: 238, Loss: 0.3350, Val: 0.6443, Test: 0.6776\n",
      "Mon Nov 22 17:20:58 2021\tEpoch: 239, Loss: 0.3044, Val: 0.5355, Test: 0.6466\n",
      "Mon Nov 22 17:21:00 2021\tEpoch: 240, Loss: 0.3121, Val: 0.6360, Test: 0.6667\n",
      "Mon Nov 22 17:21:03 2021\tEpoch: 241, Loss: 0.3070, Val: 0.5656, Test: 0.6254\n",
      "Mon Nov 22 17:21:05 2021\tEpoch: 242, Loss: 0.3060, Val: 0.5640, Test: 0.6634\n",
      "Mon Nov 22 17:21:08 2021\tEpoch: 243, Loss: 0.2982, Val: 0.5555, Test: 0.6365\n",
      "Mon Nov 22 17:21:10 2021\tEpoch: 244, Loss: 0.3207, Val: 0.6136, Test: 0.6466\n",
      "Mon Nov 22 17:21:12 2021\tEpoch: 245, Loss: 0.2998, Val: 0.5575, Test: 0.6350\n",
      "Mon Nov 22 17:21:15 2021\tEpoch: 246, Loss: 0.2925, Val: 0.5360, Test: 0.6304\n",
      "Mon Nov 22 17:21:17 2021\tEpoch: 247, Loss: 0.3400, Val: 0.5371, Test: 0.6325\n",
      "Mon Nov 22 17:21:20 2021\tEpoch: 248, Loss: 0.2942, Val: 0.5582, Test: 0.6299\n",
      "Mon Nov 22 17:21:22 2021\tEpoch: 249, Loss: 0.3306, Val: 0.5940, Test: 0.6301\n",
      "Mon Nov 22 17:21:25 2021\tEpoch: 250, Loss: 0.3421, Val: 0.5444, Test: 0.6414\n",
      "Mon Nov 22 17:21:27 2021\tEpoch: 251, Loss: 0.3089, Val: 0.5466, Test: 0.6690\n",
      "Mon Nov 22 17:21:29 2021\tEpoch: 252, Loss: 0.3115, Val: 0.5575, Test: 0.6273\n",
      "Mon Nov 22 17:21:32 2021\tEpoch: 253, Loss: 0.3023, Val: 0.5900, Test: 0.6301\n",
      "Mon Nov 22 17:21:34 2021\tEpoch: 254, Loss: 0.3144, Val: 0.5667, Test: 0.7030\n",
      "Mon Nov 22 17:21:37 2021\tEpoch: 255, Loss: 0.3399, Val: 0.5646, Test: 0.7045\n",
      "Mon Nov 22 17:21:39 2021\tEpoch: 256, Loss: 0.2968, Val: 0.5603, Test: 0.6280\n",
      "Mon Nov 22 17:21:42 2021\tEpoch: 257, Loss: 0.2957, Val: 0.5902, Test: 0.6313\n",
      "Mon Nov 22 17:21:44 2021\tEpoch: 258, Loss: 0.3409, Val: 0.5666, Test: 0.6464\n",
      "Mon Nov 22 17:21:47 2021\tEpoch: 259, Loss: 0.3037, Val: 0.5534, Test: 0.6486\n",
      "Mon Nov 22 17:21:49 2021\tEpoch: 260, Loss: 0.2817, Val: 0.5579, Test: 0.6272\n",
      "Mon Nov 22 17:21:51 2021\tEpoch: 261, Loss: 0.3022, Val: 0.5431, Test: 0.6314\n",
      "Mon Nov 22 17:21:54 2021\tEpoch: 262, Loss: 0.3045, Val: 0.5458, Test: 0.6386\n",
      "Mon Nov 22 17:21:56 2021\tEpoch: 263, Loss: 0.3374, Val: 0.5601, Test: 0.6252\n",
      "Mon Nov 22 17:21:59 2021\tEpoch: 264, Loss: 0.3241, Val: 0.5584, Test: 0.6261\n",
      "Mon Nov 22 17:22:01 2021\tEpoch: 265, Loss: 0.3135, Val: 0.5431, Test: 0.6299\n",
      "Mon Nov 22 17:22:04 2021\tEpoch: 266, Loss: 0.3113, Val: 0.5489, Test: 0.6290\n",
      "Mon Nov 22 17:22:06 2021\tEpoch: 267, Loss: 0.2834, Val: 0.5541, Test: 0.6290\n",
      "Mon Nov 22 17:22:09 2021\tEpoch: 268, Loss: 0.2985, Val: 0.5524, Test: 0.6259\n",
      "Mon Nov 22 17:22:11 2021\tEpoch: 269, Loss: 0.3208, Val: 0.5401, Test: 0.6308\n",
      "Mon Nov 22 17:22:13 2021\tEpoch: 270, Loss: 0.2866, Val: 0.5372, Test: 0.6291\n",
      "Mon Nov 22 17:22:16 2021\tEpoch: 271, Loss: 0.2781, Val: 0.5474, Test: 0.6232\n",
      "Mon Nov 22 17:22:18 2021\tEpoch: 272, Loss: 0.3084, Val: 0.5697, Test: 0.6255\n",
      "Mon Nov 22 17:22:21 2021\tEpoch: 273, Loss: 0.2868, Val: 0.5521, Test: 0.6287\n",
      "Mon Nov 22 17:22:23 2021\tEpoch: 274, Loss: 0.3039, Val: 0.5440, Test: 0.6384\n",
      "Mon Nov 22 17:22:26 2021\tEpoch: 275, Loss: 0.3071, Val: 0.5457, Test: 0.6408\n",
      "Mon Nov 22 17:22:28 2021\tEpoch: 276, Loss: 0.2783, Val: 0.5528, Test: 0.6287\n",
      "Mon Nov 22 17:22:31 2021\tEpoch: 277, Loss: 0.3055, Val: 0.5774, Test: 0.6283\n",
      "Mon Nov 22 17:22:33 2021\tEpoch: 278, Loss: 0.2825, Val: 0.5691, Test: 0.6296\n",
      "Mon Nov 22 17:22:35 2021\tEpoch: 279, Loss: 0.2790, Val: 0.5577, Test: 0.6283\n",
      "Mon Nov 22 17:22:38 2021\tEpoch: 280, Loss: 0.2903, Val: 0.5590, Test: 0.6265\n",
      "Mon Nov 22 17:22:40 2021\tEpoch: 281, Loss: 0.2984, Val: 0.5516, Test: 0.6283\n",
      "Mon Nov 22 17:22:43 2021\tEpoch: 282, Loss: 0.2756, Val: 0.5580, Test: 0.6273\n",
      "Mon Nov 22 17:22:45 2021\tEpoch: 283, Loss: 0.3025, Val: 0.5488, Test: 0.6310\n",
      "Mon Nov 22 17:22:48 2021\tEpoch: 284, Loss: 0.3094, Val: 0.5682, Test: 0.6326\n",
      "Mon Nov 22 17:22:50 2021\tEpoch: 285, Loss: 0.2906, Val: 0.5773, Test: 0.6340\n",
      "Mon Nov 22 17:22:52 2021\tEpoch: 286, Loss: 0.3072, Val: 0.5753, Test: 0.6310\n",
      "Mon Nov 22 17:22:55 2021\tEpoch: 287, Loss: 0.2985, Val: 0.5613, Test: 0.6267\n",
      "Mon Nov 22 17:22:57 2021\tEpoch: 288, Loss: 0.2839, Val: 0.5433, Test: 0.6290\n",
      "Mon Nov 22 17:23:00 2021\tEpoch: 289, Loss: 0.2822, Val: 0.5425, Test: 0.6321\n",
      "Mon Nov 22 17:23:02 2021\tEpoch: 290, Loss: 0.2721, Val: 0.5529, Test: 0.6300\n",
      "Mon Nov 22 17:23:05 2021\tEpoch: 291, Loss: 0.2919, Val: 0.5713, Test: 0.6330\n",
      "Mon Nov 22 17:23:07 2021\tEpoch: 292, Loss: 0.2831, Val: 0.5656, Test: 0.6314\n",
      "Mon Nov 22 17:23:10 2021\tEpoch: 293, Loss: 0.2756, Val: 0.5456, Test: 0.6293\n",
      "Mon Nov 22 17:23:12 2021\tEpoch: 294, Loss: 0.2962, Val: 0.5376, Test: 0.6297\n",
      "Mon Nov 22 17:23:14 2021\tEpoch: 295, Loss: 0.2801, Val: 0.5378, Test: 0.6305\n",
      "Mon Nov 22 17:23:17 2021\tEpoch: 296, Loss: 0.2965, Val: 0.5455, Test: 0.6280\n",
      "Mon Nov 22 17:23:19 2021\tEpoch: 297, Loss: 0.3106, Val: 0.5499, Test: 0.6269\n",
      "Mon Nov 22 17:23:22 2021\tEpoch: 298, Loss: 0.2747, Val: 0.5509, Test: 0.6260\n",
      "Mon Nov 22 17:23:24 2021\tEpoch: 299, Loss: 0.2966, Val: 0.5515, Test: 0.6251\n",
      "Mon Nov 22 17:23:27 2021\tEpoch: 300, Loss: 0.2746, Val: 0.5544, Test: 0.6234\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 301):\n",
    "    loss = train(train_loader)\n",
    "    test_mae = test(test_loader)\n",
    "    val_mae = test(val_loader)\n",
    "    \n",
    "    scheduler.step(val_mae)\n",
    "    # if epoch % 10 == 0:\n",
    "    print(f'{time.ctime()}\\t'\n",
    "          f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_mae:.4f}, '\n",
    "          f'Test: {test_mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d76d63-e868-4ca2-a8e0-2162a3a4bf27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep AI",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
