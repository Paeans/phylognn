{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25aa328c-fbde-4c86-837d-b109eac25828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ModuleList, Embedding\n",
    "from torch.nn import Sequential, ReLU, Linear\n",
    "from torch.nn import CrossEntropyLoss, MSELoss, L1Loss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, PNAConv, BatchNorm, global_add_pool\n",
    "\n",
    "from phylognn_model import G2Dist_GCNConv_Small\n",
    "\n",
    "from gene_graph_dataset import GeneGraphDataset\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1afe07e-f45d-4435-9efd-f7be458f83ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p, test_p = 0.7, 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c8f1633-bee2-464b-93e2-6fea1a471948",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GeneGraphDataset('dataset', 20, 20, graph_num = 1000)\n",
    "data_size = len(dataset)\n",
    "train_size, test_size = (int)(data_size * train_p), (int)(data_size * test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e2f694a-7ae1-455e-9582-8b7eb9ed67ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a95dcce3-4aaf-4d68-b153-a9581899a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle()\n",
    "train_dataset = dataset[:train_size]\n",
    "test_dataset = dataset[train_size:(train_size + test_size)]\n",
    "val_dataset = dataset[(train_size + test_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63f6283c-d51a-4747-8f1b-f437ca523abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_dataset), len(test_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddbf1dce-6f12-4fdd-be53-37271e03d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89c85b3a-1bdf-49db-8494-d04d29ba39eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_loader), len(test_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6c79a8d-bb5d-4875-80ff-6c22ece243e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = G2Dist_GCNConv_Small().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay = 0.0001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10,\n",
    "                              min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e364bfa2-7153-493c-9d74-ba7b1ae117ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = MSELoss()\n",
    "# l1_fn = L1Loss()\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "def train(train_loader):\n",
    "    model.train()\n",
    "\n",
    "    total_loss, counter = 0, 0\n",
    "    size = len(train_loader)\n",
    "    for batch, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        #loss = (out.squeeze() - data.y).abs().sum()\n",
    "        pred, y = out.softmax(axis = 1).argmax(axis = 1), data.y\n",
    "        counter += (pred == y).sum().item()\n",
    "        \n",
    "        loss = loss_fn(out, data.y)\n",
    "        \n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return total_loss / len(train_loader), counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce1b9829-772b-4e9b-a75e-1e9904d80afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_error, counter = 0, 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        \n",
    "        pred, y = out.softmax(axis = 1).argmax(axis = 1), data.y\n",
    "        counter += (pred == y).sum().item()\n",
    "        \n",
    "        # total_error += (out.squeeze() - data.y).abs().sum().item()\n",
    "        \n",
    "        total_error += loss_fn(out, data.y).item()\n",
    "        \n",
    "    return total_error / len(loader), counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fba2c20-6dee-4b91-acdb-8ee1fa2319fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='runs_g2d_10/g2dist_0020_0020_20000-small-run2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bb307-e484-493c-a60e-69911d93d554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan  1 23:44:48 2022\tEpoch: 001, Loss: 2.8585, Val: 3.6343, Test: 3.6375\n",
      "\t\t -- train_counter: 1435, test_counter:194\n",
      "Sat Jan  1 23:45:07 2022\tEpoch: 002, Loss: 2.5947, Val: 4.0441, Test: 4.0536\n",
      "\t\t -- train_counter: 2116, test_counter:266\n",
      "Sat Jan  1 23:45:25 2022\tEpoch: 003, Loss: 2.3911, Val: 2.4339, Test: 2.4465\n",
      "\t\t -- train_counter: 2616, test_counter:643\n",
      "Sat Jan  1 23:45:43 2022\tEpoch: 004, Loss: 2.1975, Val: 3.4226, Test: 3.4091\n",
      "\t\t -- train_counter: 3239, test_counter:440\n",
      "Sat Jan  1 23:46:00 2022\tEpoch: 005, Loss: 2.0538, Val: 2.7102, Test: 2.7195\n",
      "\t\t -- train_counter: 3669, test_counter:517\n",
      "Sat Jan  1 23:46:19 2022\tEpoch: 006, Loss: 1.9702, Val: 1.9627, Test: 1.9715\n",
      "\t\t -- train_counter: 3999, test_counter:1028\n",
      "Sat Jan  1 23:46:37 2022\tEpoch: 007, Loss: 1.8915, Val: 1.9090, Test: 1.9213\n",
      "\t\t -- train_counter: 4271, test_counter:1041\n",
      "Sat Jan  1 23:46:55 2022\tEpoch: 008, Loss: 1.8091, Val: 1.9911, Test: 2.0015\n",
      "\t\t -- train_counter: 4665, test_counter:895\n",
      "Sat Jan  1 23:47:16 2022\tEpoch: 009, Loss: 1.7416, Val: 1.9018, Test: 1.9113\n",
      "\t\t -- train_counter: 4948, test_counter:1066\n",
      "Sat Jan  1 23:47:36 2022\tEpoch: 010, Loss: 1.6988, Val: 1.9764, Test: 1.9895\n",
      "\t\t -- train_counter: 5202, test_counter:952\n",
      "Sat Jan  1 23:47:54 2022\tEpoch: 011, Loss: 1.6552, Val: 1.8472, Test: 1.8537\n",
      "\t\t -- train_counter: 5407, test_counter:1109\n",
      "Sat Jan  1 23:48:12 2022\tEpoch: 012, Loss: 1.5984, Val: 2.0027, Test: 2.0146\n",
      "\t\t -- train_counter: 5756, test_counter:899\n",
      "Sat Jan  1 23:48:30 2022\tEpoch: 013, Loss: 1.5470, Val: 2.0777, Test: 2.1121\n",
      "\t\t -- train_counter: 5935, test_counter:934\n",
      "Sat Jan  1 23:48:47 2022\tEpoch: 014, Loss: 1.4978, Val: 1.9995, Test: 2.0383\n",
      "\t\t -- train_counter: 6258, test_counter:945\n",
      "Sat Jan  1 23:49:06 2022\tEpoch: 015, Loss: 1.4789, Val: 1.8613, Test: 1.8723\n",
      "\t\t -- train_counter: 6432, test_counter:1082\n",
      "Sat Jan  1 23:49:23 2022\tEpoch: 016, Loss: 1.4170, Val: 1.8587, Test: 1.8711\n",
      "\t\t -- train_counter: 6772, test_counter:1087\n",
      "Sat Jan  1 23:49:41 2022\tEpoch: 017, Loss: 1.3561, Val: 2.0406, Test: 2.0643\n",
      "\t\t -- train_counter: 7089, test_counter:991\n",
      "Sat Jan  1 23:49:59 2022\tEpoch: 018, Loss: 1.2978, Val: 2.3648, Test: 2.3658\n",
      "\t\t -- train_counter: 7385, test_counter:860\n",
      "Sat Jan  1 23:50:17 2022\tEpoch: 019, Loss: 1.2622, Val: 2.1640, Test: 2.1585\n",
      "\t\t -- train_counter: 7608, test_counter:950\n",
      "Sat Jan  1 23:50:35 2022\tEpoch: 020, Loss: 1.2070, Val: 2.2501, Test: 2.2697\n",
      "\t\t -- train_counter: 7976, test_counter:925\n",
      "Sat Jan  1 23:50:53 2022\tEpoch: 021, Loss: 1.1819, Val: 1.9886, Test: 1.9833\n",
      "\t\t -- train_counter: 8158, test_counter:1094\n",
      "Sat Jan  1 23:51:10 2022\tEpoch: 022, Loss: 1.1308, Val: 2.1390, Test: 2.1470\n",
      "\t\t -- train_counter: 8365, test_counter:1011\n",
      "Sat Jan  1 23:51:28 2022\tEpoch: 023, Loss: 1.0554, Val: 2.0697, Test: 2.0744\n",
      "\t\t -- train_counter: 8793, test_counter:1067\n",
      "Sat Jan  1 23:51:46 2022\tEpoch: 024, Loss: 1.0337, Val: 2.2148, Test: 2.2163\n",
      "\t\t -- train_counter: 8914, test_counter:1011\n",
      "Sat Jan  1 23:52:03 2022\tEpoch: 025, Loss: 0.9771, Val: 2.2319, Test: 2.2202\n",
      "\t\t -- train_counter: 9157, test_counter:1043\n",
      "Sat Jan  1 23:52:21 2022\tEpoch: 026, Loss: 0.9317, Val: 2.4403, Test: 2.4474\n",
      "\t\t -- train_counter: 9458, test_counter:950\n",
      "Sat Jan  1 23:52:38 2022\tEpoch: 027, Loss: 0.8817, Val: 2.2952, Test: 2.3020\n",
      "\t\t -- train_counter: 9690, test_counter:1015\n",
      "Sat Jan  1 23:52:55 2022\tEpoch: 028, Loss: 0.8274, Val: 2.2726, Test: 2.2684\n",
      "\t\t -- train_counter: 10052, test_counter:1055\n",
      "Sat Jan  1 23:53:13 2022\tEpoch: 029, Loss: 0.7777, Val: 2.4928, Test: 2.4666\n",
      "\t\t -- train_counter: 10361, test_counter:1036\n",
      "Sat Jan  1 23:53:31 2022\tEpoch: 030, Loss: 0.7514, Val: 2.7814, Test: 2.7728\n",
      "\t\t -- train_counter: 10387, test_counter:873\n",
      "Sat Jan  1 23:53:49 2022\tEpoch: 031, Loss: 0.7024, Val: 2.4769, Test: 2.4520\n",
      "\t\t -- train_counter: 10676, test_counter:1041\n",
      "Sat Jan  1 23:54:08 2022\tEpoch: 032, Loss: 0.6644, Val: 2.5294, Test: 2.5341\n",
      "\t\t -- train_counter: 10834, test_counter:1027\n",
      "Sat Jan  1 23:54:26 2022\tEpoch: 033, Loss: 0.6611, Val: 2.5786, Test: 2.5790\n",
      "\t\t -- train_counter: 10858, test_counter:1051\n",
      "Sat Jan  1 23:54:44 2022\tEpoch: 034, Loss: 0.6244, Val: 2.9761, Test: 2.9612\n",
      "\t\t -- train_counter: 11036, test_counter:857\n",
      "Sat Jan  1 23:55:02 2022\tEpoch: 035, Loss: 0.5714, Val: 2.6086, Test: 2.6020\n",
      "\t\t -- train_counter: 11381, test_counter:1095\n",
      "Sat Jan  1 23:55:22 2022\tEpoch: 036, Loss: 0.5072, Val: 2.6434, Test: 2.6570\n",
      "\t\t -- train_counter: 11752, test_counter:1043\n",
      "Sat Jan  1 23:55:43 2022\tEpoch: 037, Loss: 0.4770, Val: 2.6912, Test: 2.6958\n",
      "\t\t -- train_counter: 11846, test_counter:1066\n",
      "Sat Jan  1 23:56:03 2022\tEpoch: 038, Loss: 0.4519, Val: 2.7839, Test: 2.7513\n",
      "\t\t -- train_counter: 11998, test_counter:1055\n",
      "Sat Jan  1 23:56:23 2022\tEpoch: 039, Loss: 0.4601, Val: 2.8386, Test: 2.8302\n",
      "\t\t -- train_counter: 11970, test_counter:1035\n",
      "Sat Jan  1 23:56:43 2022\tEpoch: 040, Loss: 0.5155, Val: 2.8069, Test: 2.7753\n",
      "\t\t -- train_counter: 11610, test_counter:1014\n",
      "Sat Jan  1 23:57:03 2022\tEpoch: 041, Loss: 0.4375, Val: 2.8624, Test: 2.8336\n",
      "\t\t -- train_counter: 12008, test_counter:1009\n",
      "Sat Jan  1 23:57:23 2022\tEpoch: 042, Loss: 0.3796, Val: 2.9950, Test: 2.9407\n",
      "\t\t -- train_counter: 12343, test_counter:1026\n",
      "Sat Jan  1 23:57:42 2022\tEpoch: 043, Loss: 0.3367, Val: 2.9468, Test: 2.9332\n",
      "\t\t -- train_counter: 12577, test_counter:1039\n",
      "Sat Jan  1 23:58:03 2022\tEpoch: 044, Loss: 0.3092, Val: 3.1956, Test: 3.1652\n",
      "\t\t -- train_counter: 12733, test_counter:1047\n",
      "Sat Jan  1 23:58:22 2022\tEpoch: 045, Loss: 0.3100, Val: 3.3554, Test: 3.3570\n",
      "\t\t -- train_counter: 12680, test_counter:902\n",
      "Sat Jan  1 23:58:42 2022\tEpoch: 046, Loss: 0.2968, Val: 3.1485, Test: 3.1174\n",
      "\t\t -- train_counter: 12761, test_counter:1046\n",
      "Sat Jan  1 23:59:03 2022\tEpoch: 047, Loss: 0.2792, Val: 3.1749, Test: 3.1263\n",
      "\t\t -- train_counter: 12803, test_counter:1028\n",
      "Sat Jan  1 23:59:24 2022\tEpoch: 048, Loss: 0.2684, Val: 3.2634, Test: 3.2191\n",
      "\t\t -- train_counter: 12888, test_counter:1047\n",
      "Sat Jan  1 23:59:46 2022\tEpoch: 049, Loss: 0.2404, Val: 3.2701, Test: 3.2198\n",
      "\t\t -- train_counter: 13009, test_counter:1053\n",
      "Sun Jan  2 00:00:08 2022\tEpoch: 050, Loss: 0.2200, Val: 3.4383, Test: 3.3998\n",
      "\t\t -- train_counter: 13112, test_counter:1045\n",
      "Sun Jan  2 00:00:30 2022\tEpoch: 051, Loss: 0.2180, Val: 3.3661, Test: 3.3133\n",
      "\t\t -- train_counter: 13114, test_counter:1091\n",
      "Sun Jan  2 00:00:52 2022\tEpoch: 052, Loss: 0.2149, Val: 3.4904, Test: 3.4152\n",
      "\t\t -- train_counter: 13094, test_counter:1070\n",
      "Sun Jan  2 00:01:14 2022\tEpoch: 053, Loss: 0.1994, Val: 3.3236, Test: 3.3114\n",
      "\t\t -- train_counter: 13148, test_counter:1045\n",
      "Sun Jan  2 00:01:37 2022\tEpoch: 054, Loss: 0.1915, Val: 3.4692, Test: 3.4310\n",
      "\t\t -- train_counter: 13237, test_counter:1033\n",
      "Sun Jan  2 00:02:00 2022\tEpoch: 055, Loss: 0.1981, Val: 3.5141, Test: 3.4687\n",
      "\t\t -- train_counter: 13177, test_counter:1022\n",
      "Sun Jan  2 00:02:21 2022\tEpoch: 056, Loss: 0.2269, Val: 3.5645, Test: 3.5290\n",
      "\t\t -- train_counter: 13024, test_counter:1039\n",
      "Sun Jan  2 00:02:43 2022\tEpoch: 057, Loss: 0.1975, Val: 3.7280, Test: 3.6458\n",
      "\t\t -- train_counter: 13175, test_counter:1046\n",
      "Sun Jan  2 00:03:05 2022\tEpoch: 058, Loss: 0.1587, Val: 3.6528, Test: 3.6360\n",
      "\t\t -- train_counter: 13380, test_counter:1056\n",
      "Sun Jan  2 00:03:27 2022\tEpoch: 059, Loss: 0.1603, Val: 3.8220, Test: 3.7712\n",
      "\t\t -- train_counter: 13362, test_counter:1042\n",
      "Sun Jan  2 00:03:50 2022\tEpoch: 060, Loss: 0.1492, Val: 3.9157, Test: 3.8482\n",
      "\t\t -- train_counter: 13425, test_counter:965\n",
      "Sun Jan  2 00:04:13 2022\tEpoch: 061, Loss: 0.1548, Val: 3.6655, Test: 3.6267\n",
      "\t\t -- train_counter: 13376, test_counter:1025\n",
      "Sun Jan  2 00:04:34 2022\tEpoch: 062, Loss: 0.1462, Val: 3.6905, Test: 3.6801\n",
      "\t\t -- train_counter: 13411, test_counter:1048\n",
      "Sun Jan  2 00:04:56 2022\tEpoch: 063, Loss: 0.1338, Val: 3.8117, Test: 3.7488\n",
      "\t\t -- train_counter: 13466, test_counter:1030\n",
      "Sun Jan  2 00:05:18 2022\tEpoch: 064, Loss: 0.1279, Val: 3.7636, Test: 3.7356\n",
      "\t\t -- train_counter: 13509, test_counter:1043\n",
      "Sun Jan  2 00:05:39 2022\tEpoch: 065, Loss: 0.1229, Val: 3.8218, Test: 3.8200\n",
      "\t\t -- train_counter: 13538, test_counter:1020\n",
      "Sun Jan  2 00:06:02 2022\tEpoch: 066, Loss: 0.1193, Val: 4.2191, Test: 4.1851\n",
      "\t\t -- train_counter: 13534, test_counter:1029\n",
      "Sun Jan  2 00:06:25 2022\tEpoch: 067, Loss: 0.1201, Val: 3.8859, Test: 3.8151\n",
      "\t\t -- train_counter: 13520, test_counter:1036\n",
      "Sun Jan  2 00:06:47 2022\tEpoch: 068, Loss: 0.1136, Val: 4.1483, Test: 4.1073\n",
      "\t\t -- train_counter: 13537, test_counter:1023\n",
      "Sun Jan  2 00:07:09 2022\tEpoch: 069, Loss: 0.1098, Val: 4.0722, Test: 4.0362\n",
      "\t\t -- train_counter: 13562, test_counter:990\n",
      "Sun Jan  2 00:07:32 2022\tEpoch: 070, Loss: 0.1101, Val: 4.0854, Test: 4.0041\n",
      "\t\t -- train_counter: 13568, test_counter:1014\n",
      "Sun Jan  2 00:07:53 2022\tEpoch: 071, Loss: 0.1086, Val: 4.0265, Test: 3.9854\n",
      "\t\t -- train_counter: 13578, test_counter:1015\n",
      "Sun Jan  2 00:08:15 2022\tEpoch: 072, Loss: 0.1054, Val: 4.7773, Test: 4.7186\n",
      "\t\t -- train_counter: 13605, test_counter:982\n",
      "Sun Jan  2 00:08:37 2022\tEpoch: 073, Loss: 0.1054, Val: 4.2512, Test: 4.1601\n",
      "\t\t -- train_counter: 13579, test_counter:1061\n",
      "Sun Jan  2 00:08:59 2022\tEpoch: 074, Loss: 0.0961, Val: 4.0952, Test: 4.0448\n",
      "\t\t -- train_counter: 13635, test_counter:1051\n",
      "Sun Jan  2 00:09:20 2022\tEpoch: 075, Loss: 0.0948, Val: 4.3270, Test: 4.3375\n",
      "\t\t -- train_counter: 13626, test_counter:946\n",
      "Sun Jan  2 00:09:42 2022\tEpoch: 076, Loss: 0.0961, Val: 4.5185, Test: 4.4840\n",
      "\t\t -- train_counter: 13629, test_counter:1000\n",
      "Sun Jan  2 00:10:04 2022\tEpoch: 077, Loss: 0.0984, Val: 4.5221, Test: 4.5146\n",
      "\t\t -- train_counter: 13609, test_counter:993\n",
      "Sun Jan  2 00:10:27 2022\tEpoch: 078, Loss: 0.1016, Val: 4.1421, Test: 4.0894\n",
      "\t\t -- train_counter: 13588, test_counter:1033\n",
      "Sun Jan  2 00:10:49 2022\tEpoch: 079, Loss: 0.1007, Val: 4.2205, Test: 4.1733\n",
      "\t\t -- train_counter: 13586, test_counter:1054\n",
      "Sun Jan  2 00:11:12 2022\tEpoch: 080, Loss: 0.0859, Val: 4.2282, Test: 4.1982\n",
      "\t\t -- train_counter: 13670, test_counter:1033\n",
      "Sun Jan  2 00:11:34 2022\tEpoch: 081, Loss: 0.0831, Val: 4.1465, Test: 4.1955\n",
      "\t\t -- train_counter: 13663, test_counter:1003\n",
      "Sun Jan  2 00:11:57 2022\tEpoch: 082, Loss: 0.0890, Val: 4.2795, Test: 4.2820\n",
      "\t\t -- train_counter: 13650, test_counter:1064\n",
      "Sun Jan  2 00:12:20 2022\tEpoch: 083, Loss: 0.0835, Val: 4.2231, Test: 4.1873\n",
      "\t\t -- train_counter: 13660, test_counter:1053\n",
      "Sun Jan  2 00:12:42 2022\tEpoch: 084, Loss: 0.0791, Val: 4.3855, Test: 4.3620\n",
      "\t\t -- train_counter: 13702, test_counter:1050\n",
      "Sun Jan  2 00:13:05 2022\tEpoch: 085, Loss: 0.0774, Val: 4.2185, Test: 4.1796\n",
      "\t\t -- train_counter: 13693, test_counter:1068\n",
      "Sun Jan  2 00:13:29 2022\tEpoch: 086, Loss: 0.0770, Val: 4.4919, Test: 4.4670\n",
      "\t\t -- train_counter: 13723, test_counter:1035\n",
      "Sun Jan  2 00:13:51 2022\tEpoch: 087, Loss: 0.0831, Val: 4.2596, Test: 4.2413\n",
      "\t\t -- train_counter: 13670, test_counter:1043\n",
      "Sun Jan  2 00:14:13 2022\tEpoch: 088, Loss: 0.0839, Val: 4.2933, Test: 4.2772\n",
      "\t\t -- train_counter: 13659, test_counter:1059\n",
      "Sun Jan  2 00:14:36 2022\tEpoch: 089, Loss: 0.0716, Val: 4.4181, Test: 4.3753\n",
      "\t\t -- train_counter: 13714, test_counter:1026\n",
      "Sun Jan  2 00:14:59 2022\tEpoch: 090, Loss: 0.0735, Val: 4.3078, Test: 4.2472\n",
      "\t\t -- train_counter: 13699, test_counter:1093\n",
      "Sun Jan  2 00:15:21 2022\tEpoch: 091, Loss: 0.0684, Val: 4.5138, Test: 4.4897\n",
      "\t\t -- train_counter: 13744, test_counter:1059\n",
      "Sun Jan  2 00:15:45 2022\tEpoch: 092, Loss: 0.0695, Val: 5.7606, Test: 5.7634\n",
      "\t\t -- train_counter: 13734, test_counter:860\n",
      "Sun Jan  2 00:16:07 2022\tEpoch: 093, Loss: 0.0675, Val: 4.4423, Test: 4.4615\n",
      "\t\t -- train_counter: 13747, test_counter:1031\n",
      "Sun Jan  2 00:16:29 2022\tEpoch: 094, Loss: 0.0658, Val: 4.3998, Test: 4.4011\n",
      "\t\t -- train_counter: 13744, test_counter:1024\n",
      "Sun Jan  2 00:16:51 2022\tEpoch: 095, Loss: 0.0658, Val: 4.6210, Test: 4.5545\n",
      "\t\t -- train_counter: 13750, test_counter:1032\n",
      "Sun Jan  2 00:17:13 2022\tEpoch: 096, Loss: 0.0742, Val: 4.4277, Test: 4.4252\n",
      "\t\t -- train_counter: 13707, test_counter:1068\n",
      "Sun Jan  2 00:17:36 2022\tEpoch: 097, Loss: 0.0697, Val: 4.8897, Test: 4.9039\n",
      "\t\t -- train_counter: 13719, test_counter:862\n",
      "Sun Jan  2 00:17:58 2022\tEpoch: 098, Loss: 0.0665, Val: 4.4928, Test: 4.4960\n",
      "\t\t -- train_counter: 13735, test_counter:1083\n",
      "Sun Jan  2 00:18:20 2022\tEpoch: 099, Loss: 0.0632, Val: 4.3970, Test: 4.4308\n",
      "\t\t -- train_counter: 13751, test_counter:1057\n",
      "Sun Jan  2 00:18:42 2022\tEpoch: 100, Loss: 0.0611, Val: 4.7839, Test: 4.7907\n",
      "\t\t -- train_counter: 13778, test_counter:1022\n",
      "Sun Jan  2 00:19:04 2022\tEpoch: 101, Loss: 0.0628, Val: 4.5201, Test: 4.5403\n",
      "\t\t -- train_counter: 13743, test_counter:1046\n",
      "Sun Jan  2 00:19:26 2022\tEpoch: 102, Loss: 0.0652, Val: 4.4545, Test: 4.4713\n",
      "\t\t -- train_counter: 13725, test_counter:1022\n",
      "Sun Jan  2 00:19:50 2022\tEpoch: 103, Loss: 0.0678, Val: 4.4987, Test: 4.5372\n",
      "\t\t -- train_counter: 13724, test_counter:1048\n",
      "Sun Jan  2 00:20:13 2022\tEpoch: 104, Loss: 0.0611, Val: 4.7719, Test: 4.6891\n",
      "\t\t -- train_counter: 13752, test_counter:1052\n",
      "Sun Jan  2 00:20:34 2022\tEpoch: 105, Loss: 0.0611, Val: 4.6071, Test: 4.5898\n",
      "\t\t -- train_counter: 13757, test_counter:1045\n",
      "Sun Jan  2 00:20:57 2022\tEpoch: 106, Loss: 0.0642, Val: 4.9075, Test: 4.8632\n",
      "\t\t -- train_counter: 13736, test_counter:928\n",
      "Sun Jan  2 00:21:20 2022\tEpoch: 107, Loss: 0.0666, Val: 4.6640, Test: 4.5624\n",
      "\t\t -- train_counter: 13706, test_counter:1023\n",
      "Sun Jan  2 00:21:43 2022\tEpoch: 108, Loss: 0.0601, Val: 7.4150, Test: 7.4357\n",
      "\t\t -- train_counter: 13753, test_counter:775\n",
      "Sun Jan  2 00:22:07 2022\tEpoch: 109, Loss: 0.0663, Val: 4.8919, Test: 4.8043\n",
      "\t\t -- train_counter: 13734, test_counter:1035\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for epoch in range(1, 1001):\n",
    "    loss, train_counter = train(train_loader)\n",
    "    test_mae, test_counter = test(test_loader)\n",
    "    val_mae, _ = test(val_loader)\n",
    "    \n",
    "    # scheduler.step(loss)\n",
    "    \n",
    "    writer.add_scalar('Loss/train', loss, epoch)\n",
    "    writer.add_scalar('Loss/test', test_mae, epoch)\n",
    "    writer.add_scalar('Loss/val', val_mae, epoch)\n",
    "    writer.add_scalar('Counter/train', train_counter/len(train_loader.dataset), epoch)\n",
    "    writer.add_scalar('Counter/test', test_counter/len(test_loader.dataset), epoch)\n",
    "    \n",
    "    print(f'{time.ctime()}\\t'\n",
    "          f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_mae:.4f}, '\n",
    "          f'Test: {test_mae:.4f}')\n",
    "    \n",
    "    print(f'\\t\\t -- train_counter: {train_counter}, test_counter:{test_counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb52095-743c-4a75-947d-7e728e87c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b771e75-9824-4e0b-9d67-b98b87acdb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tld0 = list(train_loader)[0].to(device)\n",
    "tld1 = list(test_loader)[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb990c5f-3804-447f-9ec4-ff38f99732e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res0 = model(tld0.x, tld0.edge_index, tld0.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c526c-1fa0-4498-9f4d-5510a9d5c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "res0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f709262e-4b24-4720-98d5-d3d934d80962",
   "metadata": {},
   "outputs": [],
   "source": [
    "res0.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f9b9c-5456-4492-90ed-cbe9c261d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tld0.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e86c48-0a60-48b8-ac35-8776deadb25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(res0, tld0.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b7691-9bec-4d6f-ba36-07213d45cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1Loss()(res0.argmax(axis = 1).to(torch.float), tld0.y.to(torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a3380-01eb-4ff9-9485-ca018689bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(res0.argmax(axis = 1) - tld0.y).abs().sum().item()/len(tld0.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f9c83-9877-4df9-a4d4-235e1facaabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = model(tld1.x, tld1.edge_index, tld1.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092b0b1f-3a5f-427e-abb4-3ef7723ef7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488421d-cdc8-4ea1-9617-b0fbedeab631",
   "metadata": {},
   "outputs": [],
   "source": [
    "tld1.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df503c-618d-40c8-a040-a3cc36f23817",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(res1, tld1.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e359cd-f57b-4c36-9854-e893f651c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1Loss()(res1.argmax(axis = 1).to(torch.float), tld1.y.to(torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee4bdc-004c-4a80-ab9b-322a83e784f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = [d.y.item() for d in train_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6323a-9b77-4f60-b075-c8dbfdf37c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8702e1-17db-4b05-a5d9-9a02d86bd726",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = [d.y.item() for d in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb9e24-7048-404c-9cff-01b3025ce878",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432e542f-d058-4e5c-9ccc-f2ce3828072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique([d.y.item() for d in val_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ec70e5-62dc-4e3f-b3fa-aa666e6b3089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep AI",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
