{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25aa328c-fbde-4c86-837d-b109eac25828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ModuleList, Embedding\n",
    "from torch.nn import Sequential, ReLU, Linear\n",
    "from torch.nn import CrossEntropyLoss, MSELoss, L1Loss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, PNAConv, BatchNorm, global_add_pool\n",
    "\n",
    "from phylognn_model import G2Dist_GCNConv_Global\n",
    "\n",
    "from gene_graph_dataset import GeneGraphDataset\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1afe07e-f45d-4435-9efd-f7be458f83ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p, test_p = 0.7, 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c8f1633-bee2-464b-93e2-6fea1a471948",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GeneGraphDataset('dataset', 20, 20, graph_num = 1000)\n",
    "data_size = len(dataset)\n",
    "train_size, test_size = (int)(data_size * train_p), (int)(data_size * test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e2f694a-7ae1-455e-9582-8b7eb9ed67ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a95dcce3-4aaf-4d68-b153-a9581899a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle()\n",
    "train_dataset = dataset[:train_size]\n",
    "test_dataset = dataset[train_size:(train_size + test_size)]\n",
    "val_dataset = dataset[(train_size + test_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63f6283c-d51a-4747-8f1b-f437ca523abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_dataset), len(test_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddbf1dce-6f12-4fdd-be53-37271e03d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89c85b3a-1bdf-49db-8494-d04d29ba39eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_loader), len(test_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6c79a8d-bb5d-4875-80ff-6c22ece243e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = G2Dist_GCNConv_Global().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay = 0.0001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10,\n",
    "                              min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e364bfa2-7153-493c-9d74-ba7b1ae117ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = MSELoss()\n",
    "# l1_fn = L1Loss()\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "def train(train_loader):\n",
    "    model.train()\n",
    "\n",
    "    total_loss, counter = 0, 0\n",
    "    size = len(train_loader)\n",
    "    for batch, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        #loss = (out.squeeze() - data.y).abs().sum()\n",
    "        pred, y = out.softmax(axis = 1).argmax(axis = 1), data.y\n",
    "        counter += (pred == y).sum().item()\n",
    "        \n",
    "        loss = loss_fn(out, data.y)\n",
    "        \n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return total_loss / len(train_loader), counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce1b9829-772b-4e9b-a75e-1e9904d80afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_error, counter = 0, 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        \n",
    "        pred, y = out.softmax(axis = 1).argmax(axis = 1), data.y\n",
    "        counter += (pred == y).sum().item()\n",
    "        \n",
    "        # total_error += (out.squeeze() - data.y).abs().sum().item()\n",
    "        \n",
    "        total_error += loss_fn(out, data.y).item()\n",
    "        \n",
    "    return total_error / len(loader), counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fba2c20-6dee-4b91-acdb-8ee1fa2319fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='runs_g2d_10/g2dist_0020_0020_20000-global-run1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bb307-e484-493c-a60e-69911d93d554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan  1 23:55:35 2022\tEpoch: 001, Loss: 2.9424, Val: 2.9513, Test: 2.9493\n",
      "\t\t -- train_counter: 1047, test_counter:227\n",
      "Sat Jan  1 23:55:57 2022\tEpoch: 002, Loss: 2.8420, Val: 3.0052, Test: 2.9935\n",
      "\t\t -- train_counter: 1441, test_counter:225\n",
      "Sat Jan  1 23:56:18 2022\tEpoch: 003, Loss: 2.7477, Val: 3.5015, Test: 3.4762\n",
      "\t\t -- train_counter: 1714, test_counter:225\n",
      "Sat Jan  1 23:56:39 2022\tEpoch: 004, Loss: 2.6640, Val: 3.3018, Test: 3.2778\n",
      "\t\t -- train_counter: 1950, test_counter:239\n",
      "Sat Jan  1 23:57:01 2022\tEpoch: 005, Loss: 2.6126, Val: 3.0278, Test: 3.0032\n",
      "\t\t -- train_counter: 2109, test_counter:309\n",
      "Sat Jan  1 23:57:22 2022\tEpoch: 006, Loss: 2.5778, Val: 2.9077, Test: 2.8877\n",
      "\t\t -- train_counter: 2161, test_counter:382\n",
      "Sat Jan  1 23:57:43 2022\tEpoch: 007, Loss: 2.5450, Val: 2.6969, Test: 2.6805\n",
      "\t\t -- train_counter: 2322, test_counter:542\n",
      "Sat Jan  1 23:58:05 2022\tEpoch: 008, Loss: 2.5164, Val: 2.5640, Test: 2.5535\n",
      "\t\t -- train_counter: 2422, test_counter:609\n",
      "Sat Jan  1 23:58:26 2022\tEpoch: 009, Loss: 2.4946, Val: 2.5297, Test: 2.5236\n",
      "\t\t -- train_counter: 2507, test_counter:676\n",
      "Sat Jan  1 23:58:47 2022\tEpoch: 010, Loss: 2.4771, Val: 2.5139, Test: 2.5080\n",
      "\t\t -- train_counter: 2517, test_counter:716\n",
      "Sat Jan  1 23:59:11 2022\tEpoch: 011, Loss: 2.4581, Val: 2.4960, Test: 2.4907\n",
      "\t\t -- train_counter: 2596, test_counter:699\n",
      "Sat Jan  1 23:59:34 2022\tEpoch: 012, Loss: 2.4408, Val: 2.4950, Test: 2.4874\n",
      "\t\t -- train_counter: 2653, test_counter:719\n",
      "Sat Jan  1 23:59:57 2022\tEpoch: 013, Loss: 2.4178, Val: 2.4667, Test: 2.4618\n",
      "\t\t -- train_counter: 2730, test_counter:736\n",
      "Sun Jan  2 00:00:21 2022\tEpoch: 014, Loss: 2.3922, Val: 2.4571, Test: 2.4484\n",
      "\t\t -- train_counter: 2798, test_counter:731\n",
      "Sun Jan  2 00:00:44 2022\tEpoch: 015, Loss: 2.3636, Val: 2.4923, Test: 2.4874\n",
      "\t\t -- train_counter: 2813, test_counter:688\n",
      "Sun Jan  2 00:01:07 2022\tEpoch: 016, Loss: 2.3310, Val: 2.6210, Test: 2.6242\n",
      "\t\t -- train_counter: 2919, test_counter:593\n",
      "Sun Jan  2 00:01:30 2022\tEpoch: 017, Loss: 2.2905, Val: 2.6868, Test: 2.6873\n",
      "\t\t -- train_counter: 3032, test_counter:550\n",
      "Sun Jan  2 00:01:52 2022\tEpoch: 018, Loss: 2.2450, Val: 2.4958, Test: 2.4895\n",
      "\t\t -- train_counter: 3183, test_counter:630\n",
      "Sun Jan  2 00:02:15 2022\tEpoch: 019, Loss: 2.1956, Val: 3.0403, Test: 3.0615\n",
      "\t\t -- train_counter: 3332, test_counter:340\n",
      "Sun Jan  2 00:02:37 2022\tEpoch: 020, Loss: 2.1455, Val: 2.7565, Test: 2.7609\n",
      "\t\t -- train_counter: 3447, test_counter:537\n",
      "Sun Jan  2 00:03:00 2022\tEpoch: 021, Loss: 2.0978, Val: 3.3465, Test: 3.3552\n",
      "\t\t -- train_counter: 3630, test_counter:360\n",
      "Sun Jan  2 00:03:23 2022\tEpoch: 022, Loss: 2.0517, Val: 3.2511, Test: 3.2508\n",
      "\t\t -- train_counter: 3771, test_counter:442\n",
      "Sun Jan  2 00:03:46 2022\tEpoch: 023, Loss: 2.0120, Val: 3.0593, Test: 3.0619\n",
      "\t\t -- train_counter: 3892, test_counter:502\n",
      "Sun Jan  2 00:04:10 2022\tEpoch: 024, Loss: 1.9871, Val: 2.7965, Test: 2.7960\n",
      "\t\t -- train_counter: 3855, test_counter:522\n",
      "Sun Jan  2 00:04:33 2022\tEpoch: 025, Loss: 1.9479, Val: 2.4370, Test: 2.4379\n",
      "\t\t -- train_counter: 4100, test_counter:614\n",
      "Sun Jan  2 00:04:56 2022\tEpoch: 026, Loss: 1.9138, Val: 2.1187, Test: 2.1141\n",
      "\t\t -- train_counter: 4212, test_counter:878\n",
      "Sun Jan  2 00:05:18 2022\tEpoch: 027, Loss: 1.8928, Val: 2.4711, Test: 2.4634\n",
      "\t\t -- train_counter: 4300, test_counter:651\n",
      "Sun Jan  2 00:05:42 2022\tEpoch: 028, Loss: 1.8727, Val: 2.9351, Test: 2.9296\n",
      "\t\t -- train_counter: 4331, test_counter:487\n",
      "Sun Jan  2 00:06:05 2022\tEpoch: 029, Loss: 1.8535, Val: 2.0529, Test: 2.0408\n",
      "\t\t -- train_counter: 4512, test_counter:999\n",
      "Sun Jan  2 00:06:28 2022\tEpoch: 030, Loss: 1.8221, Val: 2.2210, Test: 2.2121\n",
      "\t\t -- train_counter: 4543, test_counter:800\n",
      "Sun Jan  2 00:06:51 2022\tEpoch: 031, Loss: 1.8011, Val: 2.1871, Test: 2.1727\n",
      "\t\t -- train_counter: 4627, test_counter:826\n",
      "Sun Jan  2 00:07:15 2022\tEpoch: 032, Loss: 1.7851, Val: 1.9747, Test: 1.9597\n",
      "\t\t -- train_counter: 4748, test_counter:1020\n",
      "Sun Jan  2 00:07:38 2022\tEpoch: 033, Loss: 1.7660, Val: 2.0866, Test: 2.0662\n",
      "\t\t -- train_counter: 4778, test_counter:919\n",
      "Sun Jan  2 00:08:01 2022\tEpoch: 034, Loss: 1.7510, Val: 1.9553, Test: 1.9288\n",
      "\t\t -- train_counter: 4847, test_counter:1051\n",
      "Sun Jan  2 00:08:25 2022\tEpoch: 035, Loss: 1.7310, Val: 1.9215, Test: 1.8956\n",
      "\t\t -- train_counter: 4880, test_counter:1120\n",
      "Sun Jan  2 00:08:48 2022\tEpoch: 036, Loss: 1.7144, Val: 2.1559, Test: 2.1471\n",
      "\t\t -- train_counter: 5011, test_counter:839\n",
      "Sun Jan  2 00:09:10 2022\tEpoch: 037, Loss: 1.7025, Val: 1.9406, Test: 1.9230\n",
      "\t\t -- train_counter: 4988, test_counter:1079\n",
      "Sun Jan  2 00:09:34 2022\tEpoch: 038, Loss: 1.6878, Val: 2.1344, Test: 2.1234\n",
      "\t\t -- train_counter: 5075, test_counter:858\n",
      "Sun Jan  2 00:09:57 2022\tEpoch: 039, Loss: 1.6795, Val: 2.3484, Test: 2.3353\n",
      "\t\t -- train_counter: 5121, test_counter:718\n",
      "Sun Jan  2 00:10:19 2022\tEpoch: 040, Loss: 1.6709, Val: 1.9699, Test: 1.9373\n",
      "\t\t -- train_counter: 5101, test_counter:1079\n",
      "Sun Jan  2 00:10:41 2022\tEpoch: 041, Loss: 1.6582, Val: 2.3393, Test: 2.3308\n",
      "\t\t -- train_counter: 5175, test_counter:730\n",
      "Sun Jan  2 00:11:03 2022\tEpoch: 042, Loss: 1.6409, Val: 2.0717, Test: 2.0562\n",
      "\t\t -- train_counter: 5262, test_counter:935\n",
      "Sun Jan  2 00:11:26 2022\tEpoch: 043, Loss: 1.6393, Val: 1.9091, Test: 1.8820\n",
      "\t\t -- train_counter: 5268, test_counter:1140\n",
      "Sun Jan  2 00:11:49 2022\tEpoch: 044, Loss: 1.6199, Val: 1.8499, Test: 1.8276\n",
      "\t\t -- train_counter: 5376, test_counter:1189\n",
      "Sun Jan  2 00:12:11 2022\tEpoch: 045, Loss: 1.6086, Val: 2.1845, Test: 2.1732\n",
      "\t\t -- train_counter: 5424, test_counter:859\n",
      "Sun Jan  2 00:12:33 2022\tEpoch: 046, Loss: 1.6049, Val: 1.8493, Test: 1.8270\n",
      "\t\t -- train_counter: 5518, test_counter:1153\n",
      "Sun Jan  2 00:12:56 2022\tEpoch: 047, Loss: 1.5927, Val: 1.9453, Test: 1.9360\n",
      "\t\t -- train_counter: 5488, test_counter:1053\n",
      "Sun Jan  2 00:13:18 2022\tEpoch: 048, Loss: 1.5875, Val: 1.8616, Test: 1.8489\n",
      "\t\t -- train_counter: 5494, test_counter:1139\n",
      "Sun Jan  2 00:13:40 2022\tEpoch: 049, Loss: 1.5688, Val: 1.8563, Test: 1.8294\n",
      "\t\t -- train_counter: 5618, test_counter:1170\n",
      "Sun Jan  2 00:14:02 2022\tEpoch: 050, Loss: 1.5629, Val: 1.8579, Test: 1.8295\n",
      "\t\t -- train_counter: 5632, test_counter:1182\n",
      "Sun Jan  2 00:14:24 2022\tEpoch: 051, Loss: 1.5540, Val: 2.0208, Test: 2.0190\n",
      "\t\t -- train_counter: 5703, test_counter:989\n",
      "Sun Jan  2 00:14:46 2022\tEpoch: 052, Loss: 1.5467, Val: 1.8549, Test: 1.8367\n",
      "\t\t -- train_counter: 5715, test_counter:1150\n",
      "Sun Jan  2 00:15:09 2022\tEpoch: 053, Loss: 1.5364, Val: 1.8408, Test: 1.8156\n",
      "\t\t -- train_counter: 5776, test_counter:1195\n",
      "Sun Jan  2 00:15:30 2022\tEpoch: 054, Loss: 1.5356, Val: 2.5408, Test: 2.5496\n",
      "\t\t -- train_counter: 5723, test_counter:687\n",
      "Sun Jan  2 00:15:53 2022\tEpoch: 055, Loss: 1.5465, Val: 1.9529, Test: 1.9435\n",
      "\t\t -- train_counter: 5676, test_counter:1022\n",
      "Sun Jan  2 00:16:16 2022\tEpoch: 056, Loss: 1.5272, Val: 1.8644, Test: 1.8342\n",
      "\t\t -- train_counter: 5778, test_counter:1206\n",
      "Sun Jan  2 00:16:39 2022\tEpoch: 057, Loss: 1.5056, Val: 2.0424, Test: 2.0361\n",
      "\t\t -- train_counter: 5903, test_counter:976\n",
      "Sun Jan  2 00:17:02 2022\tEpoch: 058, Loss: 1.5035, Val: 1.8213, Test: 1.8025\n",
      "\t\t -- train_counter: 5822, test_counter:1159\n",
      "Sun Jan  2 00:17:25 2022\tEpoch: 059, Loss: 1.4934, Val: 1.9271, Test: 1.9162\n",
      "\t\t -- train_counter: 5927, test_counter:1089\n",
      "Sun Jan  2 00:17:48 2022\tEpoch: 060, Loss: 1.4886, Val: 2.0363, Test: 2.0302\n",
      "\t\t -- train_counter: 5980, test_counter:979\n",
      "Sun Jan  2 00:18:11 2022\tEpoch: 061, Loss: 1.4831, Val: 2.3345, Test: 2.3340\n",
      "\t\t -- train_counter: 6016, test_counter:822\n",
      "Sun Jan  2 00:18:34 2022\tEpoch: 062, Loss: 1.4744, Val: 1.8560, Test: 1.8494\n",
      "\t\t -- train_counter: 6044, test_counter:1145\n",
      "Sun Jan  2 00:18:57 2022\tEpoch: 063, Loss: 1.4682, Val: 2.0983, Test: 2.0596\n",
      "\t\t -- train_counter: 6159, test_counter:1046\n",
      "Sun Jan  2 00:19:20 2022\tEpoch: 064, Loss: 1.4708, Val: 2.3918, Test: 2.3371\n",
      "\t\t -- train_counter: 6073, test_counter:926\n",
      "Sun Jan  2 00:19:44 2022\tEpoch: 065, Loss: 1.4721, Val: 1.8113, Test: 1.7966\n",
      "\t\t -- train_counter: 5986, test_counter:1225\n",
      "Sun Jan  2 00:20:06 2022\tEpoch: 066, Loss: 1.4643, Val: 1.9642, Test: 1.9578\n",
      "\t\t -- train_counter: 6059, test_counter:1068\n",
      "Sun Jan  2 00:20:30 2022\tEpoch: 067, Loss: 1.4673, Val: 1.8659, Test: 1.8429\n",
      "\t\t -- train_counter: 6015, test_counter:1223\n",
      "Sun Jan  2 00:20:52 2022\tEpoch: 068, Loss: 1.4428, Val: 1.8169, Test: 1.7899\n",
      "\t\t -- train_counter: 6183, test_counter:1238\n",
      "Sun Jan  2 00:21:15 2022\tEpoch: 069, Loss: 1.4389, Val: 1.9885, Test: 1.9872\n",
      "\t\t -- train_counter: 6225, test_counter:1049\n",
      "Sun Jan  2 00:21:37 2022\tEpoch: 070, Loss: 1.4273, Val: 2.0167, Test: 1.9860\n",
      "\t\t -- train_counter: 6324, test_counter:1101\n",
      "Sun Jan  2 00:22:00 2022\tEpoch: 071, Loss: 1.4383, Val: 1.9788, Test: 1.9737\n",
      "\t\t -- train_counter: 6172, test_counter:1041\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for epoch in range(1, 1001):\n",
    "    loss, train_counter = train(train_loader)\n",
    "    test_mae, test_counter = test(test_loader)\n",
    "    val_mae, _ = test(val_loader)\n",
    "    \n",
    "    # scheduler.step(loss)\n",
    "    \n",
    "    writer.add_scalar('Loss/train', loss, epoch)\n",
    "    writer.add_scalar('Loss/test', test_mae, epoch)\n",
    "    writer.add_scalar('Loss/val', val_mae, epoch)\n",
    "    writer.add_scalar('Counter/train', train_counter/len(train_loader.dataset), epoch)\n",
    "    writer.add_scalar('Counter/test', test_counter/len(test_loader.dataset), epoch)\n",
    "    \n",
    "    print(f'{time.ctime()}\\t'\n",
    "          f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_mae:.4f}, '\n",
    "          f'Test: {test_mae:.4f}')\n",
    "    \n",
    "    print(f'\\t\\t -- train_counter: {train_counter}, test_counter:{test_counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb52095-743c-4a75-947d-7e728e87c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b771e75-9824-4e0b-9d67-b98b87acdb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tld0 = list(train_loader)[0].to(device)\n",
    "tld1 = list(test_loader)[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb990c5f-3804-447f-9ec4-ff38f99732e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res0 = model(tld0.x, tld0.edge_index, tld0.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c526c-1fa0-4498-9f4d-5510a9d5c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "res0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f709262e-4b24-4720-98d5-d3d934d80962",
   "metadata": {},
   "outputs": [],
   "source": [
    "res0.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f9b9c-5456-4492-90ed-cbe9c261d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tld0.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e86c48-0a60-48b8-ac35-8776deadb25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(res0, tld0.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b7691-9bec-4d6f-ba36-07213d45cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1Loss()(res0.argmax(axis = 1).to(torch.float), tld0.y.to(torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a3380-01eb-4ff9-9485-ca018689bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(res0.argmax(axis = 1) - tld0.y).abs().sum().item()/len(tld0.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f9c83-9877-4df9-a4d4-235e1facaabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = model(tld1.x, tld1.edge_index, tld1.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092b0b1f-3a5f-427e-abb4-3ef7723ef7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488421d-cdc8-4ea1-9617-b0fbedeab631",
   "metadata": {},
   "outputs": [],
   "source": [
    "tld1.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df503c-618d-40c8-a040-a3cc36f23817",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(res1, tld1.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e359cd-f57b-4c36-9854-e893f651c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1Loss()(res1.argmax(axis = 1).to(torch.float), tld1.y.to(torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee4bdc-004c-4a80-ab9b-322a83e784f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = [d.y.item() for d in train_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6323a-9b77-4f60-b075-c8dbfdf37c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8702e1-17db-4b05-a5d9-9a02d86bd726",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = [d.y.item() for d in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb9e24-7048-404c-9cff-01b3025ce878",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432e542f-d058-4e5c-9ccc-f2ce3828072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique([d.y.item() for d in val_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ec70e5-62dc-4e3f-b3fa-aa666e6b3089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep AI",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
