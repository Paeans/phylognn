{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25aa328c-fbde-4c86-837d-b109eac25828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ModuleList, Embedding\n",
    "from torch.nn import Sequential, ReLU, Linear\n",
    "from torch.nn import CrossEntropyLoss, MSELoss, L1Loss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, PNAConv, BatchNorm, global_add_pool\n",
    "\n",
    "from phylognn_model import G2Dist_GCNConv\n",
    "\n",
    "from gene_graph_dataset import GeneGraphDataset\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1afe07e-f45d-4435-9efd-f7be458f83ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p, test_p = 0.7, 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c8f1633-bee2-464b-93e2-6fea1a471948",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GeneGraphDataset('dataset', 20, 20, graph_num = 1000)\n",
    "data_size = len(dataset)\n",
    "train_size, test_size = (int)(data_size * train_p), (int)(data_size * test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e2f694a-7ae1-455e-9582-8b7eb9ed67ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a95dcce3-4aaf-4d68-b153-a9581899a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle()\n",
    "train_dataset = dataset[:train_size]\n",
    "test_dataset = dataset[train_size:(train_size + test_size)]\n",
    "val_dataset = dataset[(train_size + test_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63f6283c-d51a-4747-8f1b-f437ca523abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_dataset), len(test_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddbf1dce-6f12-4fdd-be53-37271e03d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89c85b3a-1bdf-49db-8494-d04d29ba39eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_loader), len(test_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6c79a8d-bb5d-4875-80ff-6c22ece243e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = G2Dist_GCNConv().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay = 0.0001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10,\n",
    "                              min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e364bfa2-7153-493c-9d74-ba7b1ae117ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = MSELoss()\n",
    "# l1_fn = L1Loss()\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "def train(train_loader):\n",
    "    model.train()\n",
    "\n",
    "    total_loss, counter = 0, 0\n",
    "    size = len(train_loader)\n",
    "    for batch, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        #loss = (out.squeeze() - data.y).abs().sum()\n",
    "        pred, y = out.softmax(axis = 1).argmax(axis = 1), data.y\n",
    "        counter += (pred == y).sum().item()\n",
    "        \n",
    "        loss = loss_fn(out, data.y)\n",
    "        \n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return total_loss / len(train_loader), counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce1b9829-772b-4e9b-a75e-1e9904d80afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_error, counter = 0, 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        \n",
    "        pred, y = out.softmax(axis = 1).argmax(axis = 1), data.y\n",
    "        counter += (pred == y).sum().item()\n",
    "        \n",
    "        # total_error += (out.squeeze() - data.y).abs().sum().item()\n",
    "        \n",
    "        total_error += loss_fn(out, data.y).item()\n",
    "        \n",
    "    return total_error / len(loader), counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fba2c20-6dee-4b91-acdb-8ee1fa2319fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='runs_g2d_10/g2dist_0020_0020_20000-gcn-run0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bb307-e484-493c-a60e-69911d93d554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan  1 23:15:28 2022\tEpoch: 001, Loss: 2.9116, Val: 8.4171, Test: 8.1133\n",
      "\t\t -- train_counter: 1011, test_counter:219\n",
      "Sat Jan  1 23:15:55 2022\tEpoch: 002, Loss: 2.7865, Val: 5.1775, Test: 5.1167\n",
      "\t\t -- train_counter: 1500, test_counter:251\n",
      "Sat Jan  1 23:16:21 2022\tEpoch: 003, Loss: 2.7110, Val: 3.2701, Test: 3.2514\n",
      "\t\t -- train_counter: 1513, test_counter:252\n",
      "Sat Jan  1 23:16:49 2022\tEpoch: 004, Loss: 2.6657, Val: 2.6872, Test: 2.6867\n",
      "\t\t -- train_counter: 1654, test_counter:516\n",
      "Sat Jan  1 23:17:16 2022\tEpoch: 005, Loss: 2.6058, Val: 2.7182, Test: 2.6926\n",
      "\t\t -- train_counter: 1806, test_counter:444\n",
      "Sat Jan  1 23:17:42 2022\tEpoch: 006, Loss: 2.4713, Val: 5.9568, Test: 5.7170\n",
      "\t\t -- train_counter: 2150, test_counter:232\n",
      "Sat Jan  1 23:18:09 2022\tEpoch: 007, Loss: 2.3882, Val: 19.6330, Test: 18.8221\n",
      "\t\t -- train_counter: 2405, test_counter:238\n",
      "Sat Jan  1 23:18:36 2022\tEpoch: 008, Loss: 2.3065, Val: 19.1274, Test: 18.3623\n",
      "\t\t -- train_counter: 2670, test_counter:259\n",
      "Sat Jan  1 23:19:02 2022\tEpoch: 009, Loss: 2.2533, Val: 11.8773, Test: 11.7209\n",
      "\t\t -- train_counter: 2711, test_counter:168\n",
      "Sat Jan  1 23:19:29 2022\tEpoch: 010, Loss: 2.1842, Val: 3.9022, Test: 3.9806\n",
      "\t\t -- train_counter: 2813, test_counter:307\n",
      "Sat Jan  1 23:19:55 2022\tEpoch: 011, Loss: 2.1253, Val: 2.4818, Test: 2.4785\n",
      "\t\t -- train_counter: 3043, test_counter:586\n",
      "Sat Jan  1 23:20:22 2022\tEpoch: 012, Loss: 2.1087, Val: 2.3316, Test: 2.3056\n",
      "\t\t -- train_counter: 3061, test_counter:707\n",
      "Sat Jan  1 23:20:48 2022\tEpoch: 013, Loss: 2.0848, Val: 2.2772, Test: 2.2662\n",
      "\t\t -- train_counter: 3243, test_counter:750\n",
      "Sat Jan  1 23:21:15 2022\tEpoch: 014, Loss: 2.0845, Val: 2.5236, Test: 2.4741\n",
      "\t\t -- train_counter: 3145, test_counter:651\n",
      "Sat Jan  1 23:21:42 2022\tEpoch: 015, Loss: 2.0745, Val: 2.2461, Test: 2.2314\n",
      "\t\t -- train_counter: 3265, test_counter:738\n",
      "Sat Jan  1 23:22:08 2022\tEpoch: 016, Loss: 2.0473, Val: 4.0149, Test: 4.0244\n",
      "\t\t -- train_counter: 3337, test_counter:346\n",
      "Sat Jan  1 23:22:35 2022\tEpoch: 017, Loss: 2.0330, Val: 2.3212, Test: 2.2892\n",
      "\t\t -- train_counter: 3318, test_counter:744\n",
      "Sat Jan  1 23:23:01 2022\tEpoch: 018, Loss: 2.0471, Val: 2.2958, Test: 2.2816\n",
      "\t\t -- train_counter: 3343, test_counter:803\n",
      "Sat Jan  1 23:23:28 2022\tEpoch: 019, Loss: 2.0191, Val: 2.0521, Test: 2.0179\n",
      "\t\t -- train_counter: 3415, test_counter:980\n",
      "Sat Jan  1 23:23:55 2022\tEpoch: 020, Loss: 2.0005, Val: 2.1570, Test: 2.1364\n",
      "\t\t -- train_counter: 3502, test_counter:877\n",
      "Sat Jan  1 23:24:21 2022\tEpoch: 021, Loss: 2.0094, Val: 2.1037, Test: 2.0757\n",
      "\t\t -- train_counter: 3488, test_counter:896\n",
      "Sat Jan  1 23:24:48 2022\tEpoch: 022, Loss: 1.9927, Val: 2.7010, Test: 2.7061\n",
      "\t\t -- train_counter: 3473, test_counter:793\n",
      "Sat Jan  1 23:25:14 2022\tEpoch: 023, Loss: 1.9994, Val: 2.0227, Test: 2.0087\n",
      "\t\t -- train_counter: 3525, test_counter:1038\n",
      "Sat Jan  1 23:25:39 2022\tEpoch: 024, Loss: 1.9912, Val: 2.1778, Test: 2.1561\n",
      "\t\t -- train_counter: 3575, test_counter:908\n",
      "Sat Jan  1 23:26:03 2022\tEpoch: 025, Loss: 1.9892, Val: 2.1498, Test: 2.1240\n",
      "\t\t -- train_counter: 3578, test_counter:941\n",
      "Sat Jan  1 23:26:28 2022\tEpoch: 026, Loss: 1.9722, Val: 2.0794, Test: 2.0694\n",
      "\t\t -- train_counter: 3577, test_counter:971\n",
      "Sat Jan  1 23:26:53 2022\tEpoch: 027, Loss: 1.9685, Val: 1.9766, Test: 1.9603\n",
      "\t\t -- train_counter: 3595, test_counter:1000\n",
      "Sat Jan  1 23:27:18 2022\tEpoch: 028, Loss: 1.9732, Val: 2.2180, Test: 2.2086\n",
      "\t\t -- train_counter: 3548, test_counter:840\n",
      "Sat Jan  1 23:27:45 2022\tEpoch: 029, Loss: 1.9562, Val: 2.1202, Test: 2.1045\n",
      "\t\t -- train_counter: 3568, test_counter:918\n",
      "Sat Jan  1 23:28:11 2022\tEpoch: 030, Loss: 1.9671, Val: 4.2140, Test: 4.2957\n",
      "\t\t -- train_counter: 3547, test_counter:406\n",
      "Sat Jan  1 23:28:36 2022\tEpoch: 031, Loss: 1.9552, Val: 2.1131, Test: 2.0984\n",
      "\t\t -- train_counter: 3642, test_counter:928\n",
      "Sat Jan  1 23:29:01 2022\tEpoch: 032, Loss: 1.9499, Val: 1.9655, Test: 1.9543\n",
      "\t\t -- train_counter: 3585, test_counter:1017\n",
      "Sat Jan  1 23:29:26 2022\tEpoch: 033, Loss: 1.9384, Val: 1.9891, Test: 1.9445\n",
      "\t\t -- train_counter: 3745, test_counter:1044\n",
      "Sat Jan  1 23:29:52 2022\tEpoch: 034, Loss: 1.9466, Val: 1.9180, Test: 1.9257\n",
      "\t\t -- train_counter: 3606, test_counter:1073\n",
      "Sat Jan  1 23:30:17 2022\tEpoch: 035, Loss: 1.9323, Val: 2.2902, Test: 2.2846\n",
      "\t\t -- train_counter: 3789, test_counter:880\n",
      "Sat Jan  1 23:30:43 2022\tEpoch: 036, Loss: 1.9399, Val: 1.9482, Test: 1.9432\n",
      "\t\t -- train_counter: 3775, test_counter:1073\n",
      "Sat Jan  1 23:31:09 2022\tEpoch: 037, Loss: 1.9288, Val: 2.0261, Test: 1.9878\n",
      "\t\t -- train_counter: 3722, test_counter:1047\n",
      "Sat Jan  1 23:31:34 2022\tEpoch: 038, Loss: 1.9350, Val: 2.4629, Test: 2.4497\n",
      "\t\t -- train_counter: 3737, test_counter:801\n",
      "Sat Jan  1 23:32:01 2022\tEpoch: 039, Loss: 1.9200, Val: 2.2517, Test: 2.2558\n",
      "\t\t -- train_counter: 3731, test_counter:906\n",
      "Sat Jan  1 23:32:27 2022\tEpoch: 040, Loss: 1.9307, Val: 2.4563, Test: 2.4178\n",
      "\t\t -- train_counter: 3727, test_counter:758\n",
      "Sat Jan  1 23:32:53 2022\tEpoch: 041, Loss: 1.9175, Val: 2.0526, Test: 2.0029\n",
      "\t\t -- train_counter: 3882, test_counter:1021\n",
      "Sat Jan  1 23:33:19 2022\tEpoch: 042, Loss: 1.9198, Val: 2.0254, Test: 1.9971\n",
      "\t\t -- train_counter: 3831, test_counter:979\n",
      "Sat Jan  1 23:33:46 2022\tEpoch: 043, Loss: 1.9114, Val: 1.9593, Test: 1.9651\n",
      "\t\t -- train_counter: 3830, test_counter:1026\n",
      "Sat Jan  1 23:34:13 2022\tEpoch: 044, Loss: 1.9136, Val: 6.1571, Test: 6.2752\n",
      "\t\t -- train_counter: 3757, test_counter:213\n",
      "Sat Jan  1 23:34:38 2022\tEpoch: 045, Loss: 1.9283, Val: 2.1574, Test: 2.1678\n",
      "\t\t -- train_counter: 3733, test_counter:945\n",
      "Sat Jan  1 23:35:04 2022\tEpoch: 046, Loss: 1.8990, Val: 2.4039, Test: 2.4483\n",
      "\t\t -- train_counter: 3870, test_counter:845\n",
      "Sat Jan  1 23:35:30 2022\tEpoch: 047, Loss: 1.9123, Val: 1.9806, Test: 1.9617\n",
      "\t\t -- train_counter: 3808, test_counter:1036\n",
      "Sat Jan  1 23:35:56 2022\tEpoch: 048, Loss: 1.9190, Val: 2.6149, Test: 2.5710\n",
      "\t\t -- train_counter: 3726, test_counter:809\n",
      "Sat Jan  1 23:36:22 2022\tEpoch: 049, Loss: 1.9060, Val: 2.4358, Test: 2.4320\n",
      "\t\t -- train_counter: 3868, test_counter:846\n",
      "Sat Jan  1 23:36:49 2022\tEpoch: 050, Loss: 1.8800, Val: 3.7454, Test: 3.7444\n",
      "\t\t -- train_counter: 3935, test_counter:675\n",
      "Sat Jan  1 23:37:14 2022\tEpoch: 051, Loss: 1.8907, Val: 3.2608, Test: 3.2779\n",
      "\t\t -- train_counter: 3903, test_counter:756\n",
      "Sat Jan  1 23:37:40 2022\tEpoch: 052, Loss: 1.8825, Val: 1.9094, Test: 1.8977\n",
      "\t\t -- train_counter: 3872, test_counter:1095\n",
      "Sat Jan  1 23:38:05 2022\tEpoch: 053, Loss: 1.8976, Val: 2.2674, Test: 2.2655\n",
      "\t\t -- train_counter: 3843, test_counter:885\n",
      "Sat Jan  1 23:38:31 2022\tEpoch: 054, Loss: 1.8954, Val: 3.7800, Test: 3.8601\n",
      "\t\t -- train_counter: 3843, test_counter:665\n",
      "Sat Jan  1 23:38:58 2022\tEpoch: 055, Loss: 1.8695, Val: 2.8321, Test: 2.8772\n",
      "\t\t -- train_counter: 3906, test_counter:746\n",
      "Sat Jan  1 23:39:26 2022\tEpoch: 056, Loss: 1.8866, Val: 2.8428, Test: 2.8311\n",
      "\t\t -- train_counter: 3880, test_counter:759\n",
      "Sat Jan  1 23:39:56 2022\tEpoch: 057, Loss: 1.8831, Val: 1.9694, Test: 1.9534\n",
      "\t\t -- train_counter: 3917, test_counter:1065\n",
      "Sat Jan  1 23:40:23 2022\tEpoch: 058, Loss: 1.8755, Val: 2.9708, Test: 2.9986\n",
      "\t\t -- train_counter: 4001, test_counter:759\n",
      "Sat Jan  1 23:40:53 2022\tEpoch: 059, Loss: 1.8700, Val: 2.1556, Test: 2.1602\n",
      "\t\t -- train_counter: 3876, test_counter:917\n",
      "Sat Jan  1 23:41:21 2022\tEpoch: 060, Loss: 1.8811, Val: 1.9176, Test: 1.8800\n",
      "\t\t -- train_counter: 3929, test_counter:1116\n",
      "Sat Jan  1 23:41:50 2022\tEpoch: 061, Loss: 1.8683, Val: 1.9039, Test: 1.9044\n",
      "\t\t -- train_counter: 3914, test_counter:1070\n",
      "Sat Jan  1 23:42:17 2022\tEpoch: 062, Loss: 1.8514, Val: 1.9152, Test: 1.9404\n",
      "\t\t -- train_counter: 4044, test_counter:1049\n",
      "Sat Jan  1 23:42:44 2022\tEpoch: 063, Loss: 1.8687, Val: 5.0396, Test: 5.1640\n",
      "\t\t -- train_counter: 3968, test_counter:639\n",
      "Sat Jan  1 23:43:12 2022\tEpoch: 064, Loss: 1.8991, Val: 1.9048, Test: 1.8866\n",
      "\t\t -- train_counter: 3779, test_counter:1117\n",
      "Sat Jan  1 23:43:40 2022\tEpoch: 065, Loss: 1.8732, Val: 2.1671, Test: 2.1574\n",
      "\t\t -- train_counter: 4041, test_counter:976\n",
      "Sat Jan  1 23:44:08 2022\tEpoch: 066, Loss: 1.8745, Val: 3.0847, Test: 3.1425\n",
      "\t\t -- train_counter: 4030, test_counter:735\n",
      "Sat Jan  1 23:44:35 2022\tEpoch: 067, Loss: 1.8444, Val: 2.3212, Test: 2.2903\n",
      "\t\t -- train_counter: 4033, test_counter:886\n",
      "Sat Jan  1 23:45:04 2022\tEpoch: 068, Loss: 1.8562, Val: 2.1231, Test: 2.1144\n",
      "\t\t -- train_counter: 4036, test_counter:981\n",
      "Sat Jan  1 23:45:33 2022\tEpoch: 069, Loss: 1.8641, Val: 2.0063, Test: 2.0078\n",
      "\t\t -- train_counter: 3989, test_counter:1018\n",
      "Sat Jan  1 23:46:02 2022\tEpoch: 070, Loss: 1.8528, Val: 1.8823, Test: 1.8765\n",
      "\t\t -- train_counter: 3998, test_counter:1140\n",
      "Sat Jan  1 23:46:30 2022\tEpoch: 071, Loss: 1.8363, Val: 2.5173, Test: 2.5640\n",
      "\t\t -- train_counter: 4039, test_counter:830\n",
      "Sat Jan  1 23:46:58 2022\tEpoch: 072, Loss: 1.8444, Val: 5.6008, Test: 5.6159\n",
      "\t\t -- train_counter: 4039, test_counter:605\n",
      "Sat Jan  1 23:47:27 2022\tEpoch: 073, Loss: 1.8588, Val: 1.8440, Test: 1.8317\n",
      "\t\t -- train_counter: 4013, test_counter:1171\n",
      "Sat Jan  1 23:47:56 2022\tEpoch: 074, Loss: 1.8414, Val: 1.8807, Test: 1.8641\n",
      "\t\t -- train_counter: 4074, test_counter:1130\n",
      "Sat Jan  1 23:48:24 2022\tEpoch: 075, Loss: 1.8233, Val: 1.8596, Test: 1.8437\n",
      "\t\t -- train_counter: 4127, test_counter:1182\n",
      "Sat Jan  1 23:48:52 2022\tEpoch: 076, Loss: 1.8416, Val: 1.9974, Test: 1.9547\n",
      "\t\t -- train_counter: 4062, test_counter:1061\n",
      "Sat Jan  1 23:49:20 2022\tEpoch: 077, Loss: 1.8426, Val: 2.1964, Test: 2.1851\n",
      "\t\t -- train_counter: 4099, test_counter:998\n",
      "Sat Jan  1 23:49:48 2022\tEpoch: 078, Loss: 1.8261, Val: 1.8536, Test: 1.8400\n",
      "\t\t -- train_counter: 4094, test_counter:1185\n",
      "Sat Jan  1 23:50:16 2022\tEpoch: 079, Loss: 1.8191, Val: 2.6344, Test: 2.6417\n",
      "\t\t -- train_counter: 4116, test_counter:870\n",
      "Sat Jan  1 23:50:45 2022\tEpoch: 080, Loss: 1.8222, Val: 1.9248, Test: 1.9020\n",
      "\t\t -- train_counter: 4135, test_counter:1105\n",
      "Sat Jan  1 23:51:13 2022\tEpoch: 081, Loss: 1.8047, Val: 1.9675, Test: 1.9666\n",
      "\t\t -- train_counter: 4211, test_counter:1062\n",
      "Sat Jan  1 23:51:41 2022\tEpoch: 082, Loss: 1.8121, Val: 2.2176, Test: 2.1995\n",
      "\t\t -- train_counter: 4147, test_counter:946\n",
      "Sat Jan  1 23:52:09 2022\tEpoch: 083, Loss: 1.8059, Val: 1.9572, Test: 1.9575\n",
      "\t\t -- train_counter: 4274, test_counter:1064\n",
      "Sat Jan  1 23:52:35 2022\tEpoch: 084, Loss: 1.8101, Val: 2.0043, Test: 1.9765\n",
      "\t\t -- train_counter: 4194, test_counter:1078\n",
      "Sat Jan  1 23:53:04 2022\tEpoch: 085, Loss: 1.8153, Val: 2.0573, Test: 2.0287\n",
      "\t\t -- train_counter: 4071, test_counter:1065\n",
      "Sat Jan  1 23:53:30 2022\tEpoch: 086, Loss: 1.8321, Val: 2.3419, Test: 2.3612\n",
      "\t\t -- train_counter: 4188, test_counter:890\n",
      "Sat Jan  1 23:53:59 2022\tEpoch: 087, Loss: 1.8200, Val: 2.4439, Test: 2.4367\n",
      "\t\t -- train_counter: 4169, test_counter:907\n",
      "Sat Jan  1 23:54:28 2022\tEpoch: 088, Loss: 1.8368, Val: 1.9472, Test: 1.9363\n",
      "\t\t -- train_counter: 4147, test_counter:1106\n",
      "Sat Jan  1 23:54:56 2022\tEpoch: 089, Loss: 1.8174, Val: 1.8468, Test: 1.8423\n",
      "\t\t -- train_counter: 4157, test_counter:1167\n",
      "Sat Jan  1 23:55:25 2022\tEpoch: 090, Loss: 1.8285, Val: 3.4482, Test: 3.3693\n",
      "\t\t -- train_counter: 4113, test_counter:750\n",
      "Sat Jan  1 23:55:58 2022\tEpoch: 091, Loss: 1.8112, Val: 1.8595, Test: 1.8550\n",
      "\t\t -- train_counter: 4148, test_counter:1153\n",
      "Sat Jan  1 23:56:28 2022\tEpoch: 092, Loss: 1.8179, Val: 2.3961, Test: 2.3860\n",
      "\t\t -- train_counter: 4109, test_counter:909\n",
      "Sat Jan  1 23:56:58 2022\tEpoch: 093, Loss: 1.8025, Val: 2.5277, Test: 2.5216\n",
      "\t\t -- train_counter: 4206, test_counter:834\n",
      "Sat Jan  1 23:57:28 2022\tEpoch: 094, Loss: 1.8057, Val: 2.0745, Test: 2.0396\n",
      "\t\t -- train_counter: 4223, test_counter:1082\n",
      "Sat Jan  1 23:57:58 2022\tEpoch: 095, Loss: 1.7971, Val: 1.8970, Test: 1.8661\n",
      "\t\t -- train_counter: 4228, test_counter:1209\n",
      "Sat Jan  1 23:58:29 2022\tEpoch: 096, Loss: 1.7974, Val: 1.9574, Test: 1.9229\n",
      "\t\t -- train_counter: 4222, test_counter:1136\n",
      "Sat Jan  1 23:58:59 2022\tEpoch: 097, Loss: 1.7892, Val: 1.8754, Test: 1.8733\n",
      "\t\t -- train_counter: 4289, test_counter:1146\n",
      "Sat Jan  1 23:59:34 2022\tEpoch: 098, Loss: 1.7771, Val: 1.8709, Test: 1.9041\n",
      "\t\t -- train_counter: 4257, test_counter:1128\n",
      "Sun Jan  2 00:00:09 2022\tEpoch: 099, Loss: 1.7901, Val: 5.7659, Test: 5.7489\n",
      "\t\t -- train_counter: 4275, test_counter:410\n",
      "Sun Jan  2 00:00:43 2022\tEpoch: 100, Loss: 1.8041, Val: 2.1153, Test: 2.0948\n",
      "\t\t -- train_counter: 4224, test_counter:964\n",
      "Sun Jan  2 00:01:17 2022\tEpoch: 101, Loss: 1.7891, Val: 2.0790, Test: 2.0732\n",
      "\t\t -- train_counter: 4277, test_counter:1035\n",
      "Sun Jan  2 00:01:51 2022\tEpoch: 102, Loss: 1.7986, Val: 1.8464, Test: 1.8524\n",
      "\t\t -- train_counter: 4213, test_counter:1174\n",
      "Sun Jan  2 00:02:24 2022\tEpoch: 103, Loss: 1.7842, Val: 1.9044, Test: 1.9076\n",
      "\t\t -- train_counter: 4302, test_counter:1094\n",
      "Sun Jan  2 00:02:59 2022\tEpoch: 104, Loss: 1.7809, Val: 2.1336, Test: 2.1703\n",
      "\t\t -- train_counter: 4255, test_counter:1005\n",
      "Sun Jan  2 00:03:32 2022\tEpoch: 105, Loss: 1.7840, Val: 2.7174, Test: 2.7051\n",
      "\t\t -- train_counter: 4352, test_counter:848\n",
      "Sun Jan  2 00:04:06 2022\tEpoch: 106, Loss: 1.7824, Val: 2.0927, Test: 2.0848\n",
      "\t\t -- train_counter: 4305, test_counter:997\n",
      "Sun Jan  2 00:04:40 2022\tEpoch: 107, Loss: 1.7872, Val: 1.8412, Test: 1.8308\n",
      "\t\t -- train_counter: 4352, test_counter:1183\n",
      "Sun Jan  2 00:05:13 2022\tEpoch: 108, Loss: 1.7849, Val: 1.9025, Test: 1.8891\n",
      "\t\t -- train_counter: 4251, test_counter:1169\n",
      "Sun Jan  2 00:05:48 2022\tEpoch: 109, Loss: 1.7879, Val: 2.7410, Test: 2.7802\n",
      "\t\t -- train_counter: 4198, test_counter:773\n",
      "Sun Jan  2 00:06:21 2022\tEpoch: 110, Loss: 1.7800, Val: 2.2612, Test: 2.2674\n",
      "\t\t -- train_counter: 4308, test_counter:925\n",
      "Sun Jan  2 00:06:55 2022\tEpoch: 111, Loss: 1.7844, Val: 1.8633, Test: 1.8583\n",
      "\t\t -- train_counter: 4257, test_counter:1155\n",
      "Sun Jan  2 00:07:27 2022\tEpoch: 112, Loss: 1.7803, Val: 1.8094, Test: 1.8108\n",
      "\t\t -- train_counter: 4361, test_counter:1224\n",
      "Sun Jan  2 00:08:01 2022\tEpoch: 113, Loss: 1.7885, Val: 2.4009, Test: 2.4171\n",
      "\t\t -- train_counter: 4223, test_counter:929\n",
      "Sun Jan  2 00:08:34 2022\tEpoch: 114, Loss: 1.7946, Val: 1.9185, Test: 1.8915\n",
      "\t\t -- train_counter: 4267, test_counter:1222\n",
      "Sun Jan  2 00:09:08 2022\tEpoch: 115, Loss: 1.7913, Val: 1.8757, Test: 1.8653\n",
      "\t\t -- train_counter: 4196, test_counter:1120\n",
      "Sun Jan  2 00:09:42 2022\tEpoch: 116, Loss: 1.7827, Val: 1.8671, Test: 1.9005\n",
      "\t\t -- train_counter: 4269, test_counter:1141\n",
      "Sun Jan  2 00:10:18 2022\tEpoch: 117, Loss: 1.7894, Val: 1.8809, Test: 1.8839\n",
      "\t\t -- train_counter: 4318, test_counter:1205\n",
      "Sun Jan  2 00:10:54 2022\tEpoch: 118, Loss: 1.7797, Val: 1.8139, Test: 1.8151\n",
      "\t\t -- train_counter: 4302, test_counter:1184\n",
      "Sun Jan  2 00:11:30 2022\tEpoch: 119, Loss: 1.7643, Val: 1.7978, Test: 1.8049\n",
      "\t\t -- train_counter: 4404, test_counter:1190\n",
      "Sun Jan  2 00:12:05 2022\tEpoch: 120, Loss: 1.7736, Val: 1.8243, Test: 1.8073\n",
      "\t\t -- train_counter: 4336, test_counter:1206\n",
      "Sun Jan  2 00:12:39 2022\tEpoch: 121, Loss: 1.7848, Val: 1.7868, Test: 1.7937\n",
      "\t\t -- train_counter: 4205, test_counter:1187\n",
      "Sun Jan  2 00:13:13 2022\tEpoch: 122, Loss: 1.7613, Val: 1.8823, Test: 1.8633\n",
      "\t\t -- train_counter: 4449, test_counter:1150\n",
      "Sun Jan  2 00:13:47 2022\tEpoch: 123, Loss: 1.7691, Val: 1.9014, Test: 1.8931\n",
      "\t\t -- train_counter: 4322, test_counter:1162\n",
      "Sun Jan  2 00:14:21 2022\tEpoch: 124, Loss: 1.7669, Val: 2.5267, Test: 2.5729\n",
      "\t\t -- train_counter: 4327, test_counter:882\n",
      "Sun Jan  2 00:14:56 2022\tEpoch: 125, Loss: 1.7814, Val: 2.5613, Test: 2.5879\n",
      "\t\t -- train_counter: 4362, test_counter:925\n",
      "Sun Jan  2 00:15:31 2022\tEpoch: 126, Loss: 1.7624, Val: 2.3925, Test: 2.4218\n",
      "\t\t -- train_counter: 4353, test_counter:918\n",
      "Sun Jan  2 00:16:05 2022\tEpoch: 127, Loss: 1.7669, Val: 2.6265, Test: 2.7080\n",
      "\t\t -- train_counter: 4289, test_counter:822\n",
      "Sun Jan  2 00:16:41 2022\tEpoch: 128, Loss: 1.7598, Val: 1.9348, Test: 1.9389\n",
      "\t\t -- train_counter: 4407, test_counter:1168\n",
      "Sun Jan  2 00:17:15 2022\tEpoch: 129, Loss: 1.7677, Val: 1.8495, Test: 1.8310\n",
      "\t\t -- train_counter: 4357, test_counter:1168\n",
      "Sun Jan  2 00:17:50 2022\tEpoch: 130, Loss: 1.7610, Val: 2.2809, Test: 2.2990\n",
      "\t\t -- train_counter: 4331, test_counter:922\n",
      "Sun Jan  2 00:18:24 2022\tEpoch: 131, Loss: 1.7670, Val: 2.7204, Test: 2.6739\n",
      "\t\t -- train_counter: 4315, test_counter:721\n",
      "Sun Jan  2 00:18:58 2022\tEpoch: 132, Loss: 1.7896, Val: 2.1352, Test: 2.1482\n",
      "\t\t -- train_counter: 4261, test_counter:991\n",
      "Sun Jan  2 00:19:32 2022\tEpoch: 133, Loss: 1.7904, Val: 2.0360, Test: 2.0155\n",
      "\t\t -- train_counter: 4256, test_counter:1120\n",
      "Sun Jan  2 00:20:07 2022\tEpoch: 134, Loss: 1.7819, Val: 1.7910, Test: 1.7997\n",
      "\t\t -- train_counter: 4348, test_counter:1183\n",
      "Sun Jan  2 00:20:41 2022\tEpoch: 135, Loss: 1.7678, Val: 2.0850, Test: 2.0840\n",
      "\t\t -- train_counter: 4348, test_counter:1016\n",
      "Sun Jan  2 00:21:15 2022\tEpoch: 136, Loss: 1.7690, Val: 1.8623, Test: 1.8474\n",
      "\t\t -- train_counter: 4359, test_counter:1189\n",
      "Sun Jan  2 00:21:49 2022\tEpoch: 137, Loss: 1.7823, Val: 2.7760, Test: 2.8058\n",
      "\t\t -- train_counter: 4357, test_counter:865\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for epoch in range(1, 1001):\n",
    "    loss, train_counter = train(train_loader)\n",
    "    test_mae, test_counter = test(test_loader)\n",
    "    val_mae, _ = test(val_loader)\n",
    "    \n",
    "    # scheduler.step(loss)\n",
    "    \n",
    "    writer.add_scalar('Loss/train', loss, epoch)\n",
    "    writer.add_scalar('Loss/test', test_mae, epoch)\n",
    "    writer.add_scalar('Loss/val', val_mae, epoch)\n",
    "    writer.add_scalar('Counter/train', train_counter/len(train_loader.dataset), epoch)\n",
    "    writer.add_scalar('Counter/test', test_counter/len(test_loader.dataset), epoch)\n",
    "    \n",
    "    print(f'{time.ctime()}\\t'\n",
    "          f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_mae:.4f}, '\n",
    "          f'Test: {test_mae:.4f}')\n",
    "    \n",
    "    print(f'\\t\\t -- train_counter: {train_counter}, test_counter:{test_counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb52095-743c-4a75-947d-7e728e87c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b771e75-9824-4e0b-9d67-b98b87acdb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tld0 = list(train_loader)[0].to(device)\n",
    "tld1 = list(test_loader)[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb990c5f-3804-447f-9ec4-ff38f99732e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res0 = model(tld0.x, tld0.edge_index, tld0.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c526c-1fa0-4498-9f4d-5510a9d5c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "res0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f709262e-4b24-4720-98d5-d3d934d80962",
   "metadata": {},
   "outputs": [],
   "source": [
    "res0.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f9b9c-5456-4492-90ed-cbe9c261d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tld0.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e86c48-0a60-48b8-ac35-8776deadb25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(res0, tld0.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b7691-9bec-4d6f-ba36-07213d45cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1Loss()(res0.argmax(axis = 1).to(torch.float), tld0.y.to(torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a3380-01eb-4ff9-9485-ca018689bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(res0.argmax(axis = 1) - tld0.y).abs().sum().item()/len(tld0.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f9c83-9877-4df9-a4d4-235e1facaabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = model(tld1.x, tld1.edge_index, tld1.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092b0b1f-3a5f-427e-abb4-3ef7723ef7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488421d-cdc8-4ea1-9617-b0fbedeab631",
   "metadata": {},
   "outputs": [],
   "source": [
    "tld1.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df503c-618d-40c8-a040-a3cc36f23817",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(res1, tld1.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e359cd-f57b-4c36-9854-e893f651c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1Loss()(res1.argmax(axis = 1).to(torch.float), tld1.y.to(torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee4bdc-004c-4a80-ab9b-322a83e784f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = [d.y.item() for d in train_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6323a-9b77-4f60-b075-c8dbfdf37c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8702e1-17db-4b05-a5d9-9a02d86bd726",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = [d.y.item() for d in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb9e24-7048-404c-9cff-01b3025ce878",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432e542f-d058-4e5c-9ccc-f2ce3828072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique([d.y.item() for d in val_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ec70e5-62dc-4e3f-b3fa-aa666e6b3089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep AI",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
