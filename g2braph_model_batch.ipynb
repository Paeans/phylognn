{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8832435c-cc48-40da-b226-548b3281c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "from phylognn_model import G2Braph_GCNConv\n",
    "from gene_graph_dataset import G2BraphDataset\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11a6854-b123-4f21-876b-75717250c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p, test_p = 0.7, 0.2\n",
    "train_batch = 25\n",
    "test_batch, val_batch = 8, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab9e9f86-5660-485c-921d-40e6eeaa2437",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = G2BraphDataset('dataset_g2b', 10, 10).shuffle()\n",
    "data_size = len(dataset)\n",
    "train_size, test_size = (int)(data_size * train_p), (int)(data_size * test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5d4f38f-0069-4a96-ab1c-2c72e2a9b810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b428c27-6f37-480e-ae50-d86dfc839c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[:train_size]\n",
    "test_dataset = dataset[train_size:(train_size + test_size)]\n",
    "val_dataset = dataset[(train_size + test_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "394976a2-812c-4add-9041-48d897d7caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=train_batch, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=val_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ad402c1-6d41-412b-8a9d-fe71da146b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deg = torch.zeros(5, dtype=torch.long)\n",
    "# for data in train_dataset:\n",
    "#     d = degree(data.edge_index[1].type(torch.int64), \n",
    "#                num_nodes=data.num_nodes, dtype=torch.long)\n",
    "#     deg += torch.bincount(d, minlength=deg.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebce5584-e4e5-4963-8d64-f7822cf8757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpuid = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb03c710-bda4-492e-86e8-3696cd51de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:' + str(gpuid) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = G2Braph_GCNConv().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20,\n",
    "                              min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3c2a368-5a64-4f2f-8c7b-a812984f0c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for data in train_dataset:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        res = model(data.x, data.edge_index)\n",
    "        loss = F.binary_cross_entropy(res.squeeze(), data.node_label.to(torch.float))\n",
    "        loss.backward()\n",
    "        \n",
    "        total_loss += loss\n",
    "        optimizer.step()\n",
    "        \n",
    "    return total_loss / len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "638752f2-dc13-497c-b692-07f34bfdb141",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(test_dataset):\n",
    "    model.eval()\n",
    "    \n",
    "    tloss = 0\n",
    "    for data in test_dataset:\n",
    "        data = data.to(device)\n",
    "        res = model(data.x, data.edge_index)\n",
    "        \n",
    "        # y, pred = data.node_label.cpu().numpy(), res.squeeze().cpu().numpy()\n",
    "        \n",
    "        tloss += F.binary_cross_entropy(res.squeeze(), data.node_label.to(torch.float))\n",
    "        \n",
    "    return tloss / len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f58ad428-cdd3-4997-a50a-8a911215f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(test_dataset):\n",
    "    model.eval()\n",
    "    \n",
    "    auc, ap, counter = 0, 0, 0\n",
    "    for data in test_dataset:\n",
    "        data = data.to(device)\n",
    "        res = model(data.x, data.edge_index)\n",
    "        \n",
    "        y, pred = data.node_label.cpu().numpy(), res.squeeze().cpu().numpy()\n",
    "        if y.sum() == 0 or y.sum() == len(y):\n",
    "            continue\n",
    "        counter += 1\n",
    "        auc += roc_auc_score(y, pred)\n",
    "        ap += average_precision_score(y, pred)\n",
    "        \n",
    "    return auc/counter, ap/counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40d1caae-4b52-4a14-a920-4fec3e93596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='runs_g2b_10/1000_gcn_run2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc7b73-2132-4f35-97b1-b05117d34b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 28 23:30:05 2021  Epoch: 0050, train Loss: 0.2201, val Loss: 0.3685, auc: 0.9215, ap: 0.9504\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 1001):\n",
    "    train_loss = train(train_loader)\n",
    "    val_loss = validate(val_loader)\n",
    "    \n",
    "    if epoch > 500:\n",
    "        scheduler.step(val_loss)\n",
    "    \n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/validate', val_loss, epoch)\n",
    "    \n",
    "    auc, ap = test(test_dataset)\n",
    "    writer.add_scalar('AUC/validate', auc, epoch)\n",
    "    writer.add_scalar('AP/validate', ap, epoch)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f'{time.ctime()}  '\n",
    "              f'Epoch: {epoch:04d}, train Loss: {train_loss:.4f}, '\n",
    "              f'val Loss: {val_loss:.4f}, auc: {auc:.4f}, '\n",
    "              f'ap: {ap:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d2d07-e5c8-4d76-8813-ec4d8b463f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12e310-477b-4396-a833-f189608bcb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ea1ca-1de5-4c16-b664-aef5bd1143f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep AI",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
