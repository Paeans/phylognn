{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8832435c-cc48-40da-b226-548b3281c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "from phylognn_model import G2Braph\n",
    "from gene_graph_dataset import G2BraphDataset\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11a6854-b123-4f21-876b-75717250c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p, test_p = 0.7, 0.2\n",
    "train_batch = 16\n",
    "test_batch, val_batch = 8, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab9e9f86-5660-485c-921d-40e6eeaa2437",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = G2BraphDataset('dataset_g2b', 1000, 10).shuffle()\n",
    "data_size = len(dataset)\n",
    "train_size, test_size = (int)(data_size * train_p), (int)(data_size * test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b428c27-6f37-480e-ae50-d86dfc839c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[:train_size]\n",
    "test_dataset = dataset[train_size:(train_size + test_size)]\n",
    "val_dataset = dataset[(train_size + test_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "394976a2-812c-4add-9041-48d897d7caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=train_batch, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=val_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ad402c1-6d41-412b-8a9d-fe71da146b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = torch.zeros(5, dtype=torch.long)\n",
    "for data in train_dataset:\n",
    "    d = degree(data.edge_index[1].type(torch.int64), \n",
    "               num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce5584-e4e5-4963-8d64-f7822cf8757d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb03c710-bda4-492e-86e8-3696cd51de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = G2Braph(deg).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20,\n",
    "                              min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3c2a368-5a64-4f2f-8c7b-a812984f0c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for data in train_dataset:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        res = model(data.x, data.edge_index, None, None)\n",
    "        loss = F.binary_cross_entropy(res.squeeze(), data.node_label.to(torch.float))\n",
    "        loss.backward()\n",
    "        \n",
    "        total_loss += loss\n",
    "        optimizer.step()\n",
    "        \n",
    "    return total_loss / len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638752f2-dc13-497c-b692-07f34bfdb141",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(test_dataset):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss, auc, ap = 0, 0, 0\n",
    "    for data in test_dataset:\n",
    "        data = data.to(device)\n",
    "        res = model(data.x, data.edge_index, None, None)\n",
    "        \n",
    "        y, pred = data.node_label.cpu().numpy(), res.squeeze().cpu().numpy()\n",
    "        \n",
    "        total_loss += F.binary_cross_entropy(res.squeeze(), data.node_label.to(torch.float))\n",
    "        auc += roc_auc_score(y, pred)\n",
    "        ap += average_precision_score(y, pred)\n",
    "        \n",
    "    return total_loss / len(test_dataset), auc / len(test_dataset), ap / len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f58ad428-cdd3-4997-a50a-8a911215f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(test_dataset):\n",
    "    model.eval()\n",
    "    \n",
    "    loss, auc, ap = [], [], []\n",
    "    for data in test_dataset:\n",
    "        data = data.to(device)\n",
    "        res = model(data.x, data.edge_index, None, None)\n",
    "        \n",
    "        y, pred = data.node_label.cpu().numpy(), res.squeeze().cpu().numpy()\n",
    "        \n",
    "        loss.apppend(F.binary_cross_entropy(res.squeeze(), data.node_label.to(torch.float)))\n",
    "        auc.append(roc_auc_score(y, pred))\n",
    "        ap.append(average_precision_score(y, pred))\n",
    "        \n",
    "    return loss, auc, ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40d1caae-4b52-4a14-a920-4fec3e93596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='runs/g2braph_batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc7b73-2132-4f35-97b1-b05117d34b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 21 21:33:32 2021  Epoch: 0001, Train Loss: 0.3173, Val Loss: 0.1091, Val AUC: 0.8862, Val AP: 0.6745\n",
      "Tue Dec 21 21:34:17 2021  Epoch: 0002, Train Loss: 0.0628, Val Loss: 0.0401, Val AUC: 0.8923, Val AP: 0.6988\n",
      "Tue Dec 21 21:35:02 2021  Epoch: 0003, Train Loss: 0.0353, Val Loss: 0.0312, Val AUC: 0.8201, Val AP: 0.6944\n",
      "Tue Dec 21 21:35:47 2021  Epoch: 0004, Train Loss: 0.0317, Val Loss: 0.0287, Val AUC: 0.8223, Val AP: 0.6949\n",
      "Tue Dec 21 21:36:32 2021  Epoch: 0005, Train Loss: 0.0305, Val Loss: 0.0281, Val AUC: 0.9082, Val AP: 0.7093\n",
      "Tue Dec 21 21:37:17 2021  Epoch: 0006, Train Loss: 0.0290, Val Loss: 0.0285, Val AUC: 0.8959, Val AP: 0.7065\n",
      "Tue Dec 21 21:38:02 2021  Epoch: 0007, Train Loss: 0.0282, Val Loss: 0.0264, Val AUC: 0.9164, Val AP: 0.7089\n",
      "Tue Dec 21 21:38:46 2021  Epoch: 0008, Train Loss: 0.0269, Val Loss: 0.0261, Val AUC: 0.9196, Val AP: 0.7138\n",
      "Tue Dec 21 21:39:32 2021  Epoch: 0009, Train Loss: 0.0259, Val Loss: 0.0261, Val AUC: 0.9203, Val AP: 0.7169\n",
      "Tue Dec 21 21:40:16 2021  Epoch: 0010, Train Loss: 0.0245, Val Loss: 0.0251, Val AUC: 0.9263, Val AP: 0.7144\n",
      "Tue Dec 21 21:41:02 2021  Epoch: 0011, Train Loss: 0.0238, Val Loss: 0.0249, Val AUC: 0.9265, Val AP: 0.7143\n",
      "Tue Dec 21 21:41:46 2021  Epoch: 0012, Train Loss: 0.0232, Val Loss: 0.0256, Val AUC: 0.9231, Val AP: 0.7087\n",
      "Tue Dec 21 21:42:32 2021  Epoch: 0013, Train Loss: 0.0220, Val Loss: 0.0236, Val AUC: 0.9291, Val AP: 0.7217\n",
      "Tue Dec 21 21:43:16 2021  Epoch: 0014, Train Loss: 0.0186, Val Loss: 0.0226, Val AUC: 0.9272, Val AP: 0.7208\n",
      "Tue Dec 21 21:44:02 2021  Epoch: 0015, Train Loss: 0.0173, Val Loss: 0.0220, Val AUC: 0.9255, Val AP: 0.7047\n",
      "Tue Dec 21 21:44:46 2021  Epoch: 0016, Train Loss: 0.0171, Val Loss: 0.0218, Val AUC: 0.9281, Val AP: 0.7161\n",
      "Tue Dec 21 21:45:31 2021  Epoch: 0017, Train Loss: 0.0166, Val Loss: 0.0228, Val AUC: 0.9208, Val AP: 0.6955\n",
      "Tue Dec 21 21:46:16 2021  Epoch: 0018, Train Loss: 0.0165, Val Loss: 0.0230, Val AUC: 0.9212, Val AP: 0.7032\n",
      "Tue Dec 21 21:47:01 2021  Epoch: 0019, Train Loss: 0.0162, Val Loss: 0.0260, Val AUC: 0.9313, Val AP: 0.7160\n",
      "Tue Dec 21 21:47:46 2021  Epoch: 0020, Train Loss: 0.0161, Val Loss: 0.0221, Val AUC: 0.9236, Val AP: 0.7103\n",
      "Tue Dec 21 21:48:31 2021  Epoch: 0021, Train Loss: 0.0159, Val Loss: 0.0231, Val AUC: 0.9184, Val AP: 0.6979\n",
      "Tue Dec 21 21:49:16 2021  Epoch: 0022, Train Loss: 0.0157, Val Loss: 0.0232, Val AUC: 0.9194, Val AP: 0.7035\n",
      "Tue Dec 21 21:50:01 2021  Epoch: 0023, Train Loss: 0.0155, Val Loss: 0.0247, Val AUC: 0.9157, Val AP: 0.6976\n",
      "Tue Dec 21 21:50:46 2021  Epoch: 0024, Train Loss: 0.0153, Val Loss: 0.0243, Val AUC: 0.9141, Val AP: 0.6935\n",
      "Tue Dec 21 21:51:31 2021  Epoch: 0025, Train Loss: 0.0151, Val Loss: 0.0245, Val AUC: 0.9141, Val AP: 0.6969\n",
      "Tue Dec 21 21:52:16 2021  Epoch: 0026, Train Loss: 0.0151, Val Loss: 0.0247, Val AUC: 0.9143, Val AP: 0.6984\n",
      "Tue Dec 21 21:53:01 2021  Epoch: 0027, Train Loss: 0.0152, Val Loss: 0.0244, Val AUC: 0.9221, Val AP: 0.6984\n",
      "Tue Dec 21 21:53:46 2021  Epoch: 0028, Train Loss: 0.0150, Val Loss: 0.0232, Val AUC: 0.9230, Val AP: 0.7114\n",
      "Tue Dec 21 21:54:31 2021  Epoch: 0029, Train Loss: 0.0147, Val Loss: 0.0238, Val AUC: 0.9174, Val AP: 0.7002\n",
      "Tue Dec 21 21:55:16 2021  Epoch: 0030, Train Loss: 0.0146, Val Loss: 0.0228, Val AUC: 0.9201, Val AP: 0.7118\n",
      "Tue Dec 21 21:56:01 2021  Epoch: 0031, Train Loss: 0.0144, Val Loss: 0.0268, Val AUC: 0.9081, Val AP: 0.6876\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 2001):\n",
    "    train_loss = train(train_loader)\n",
    "    val_loss, val_auc, val_ap = validate(val_loader)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/validate', val_loss, epoch)\n",
    "    writer.add_scalar('AUC/validate', val_auc, epoch)\n",
    "    writer.add_scalar('AP/validate', val_ap, epoch)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f'{time.ctime()}  '\n",
    "              f'Epoch: {epoch:04d}, Train Loss: {train_loss:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}, '\n",
    "              f'Val AP: {val_ap:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b56a7-e2be-4976-a391-3001fdb66c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_auc, test_ap = test(test_dataset.shuffle())\n",
    "print(f'Test Loss: {train_loss:.4f}, Test AUC: {test_auc:.4f}, '\n",
    "          f'Test AP: {test_ap:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d2d07-e5c8-4d76-8813-ec4d8b463f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep AI",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
