{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8832435c-cc48-40da-b226-548b3281c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "from phylognn_model import G2Braph\n",
    "from gene_graph_dataset import G2BraphDataset\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11a6854-b123-4f21-876b-75717250c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p, test_p = 0.7, 0.2\n",
    "train_batch = 25\n",
    "test_batch, val_batch = 8, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab9e9f86-5660-485c-921d-40e6eeaa2437",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = G2BraphDataset('dataset_g2b', 1000, 10).shuffle()\n",
    "data_size = len(dataset)\n",
    "train_size, test_size = (int)(data_size * train_p), (int)(data_size * test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b428c27-6f37-480e-ae50-d86dfc839c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[:train_size]\n",
    "test_dataset = dataset[train_size:(train_size + test_size)]\n",
    "val_dataset = dataset[(train_size + test_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "394976a2-812c-4add-9041-48d897d7caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=train_batch, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=val_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ad402c1-6d41-412b-8a9d-fe71da146b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = torch.zeros(5, dtype=torch.long)\n",
    "for data in train_dataset:\n",
    "    d = degree(data.edge_index[1].type(torch.int64), \n",
    "               num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce5584-e4e5-4963-8d64-f7822cf8757d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb03c710-bda4-492e-86e8-3696cd51de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = G2Braph(deg).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20,\n",
    "                              min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3c2a368-5a64-4f2f-8c7b-a812984f0c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for data in train_dataset:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        res = model(data.x, data.edge_index, None, None)\n",
    "        loss = F.binary_cross_entropy(res.squeeze(), data.node_label.to(torch.float))\n",
    "        loss.backward()\n",
    "        \n",
    "        total_loss += loss\n",
    "        optimizer.step()\n",
    "        \n",
    "    return total_loss / len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "638752f2-dc13-497c-b692-07f34bfdb141",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(test_dataset):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss, auc, ap = 0, 0, 0\n",
    "    for data in test_dataset:\n",
    "        data = data.to(device)\n",
    "        res = model(data.x, data.edge_index, None, None)\n",
    "        \n",
    "        y, pred = data.node_label.cpu().numpy(), res.squeeze().cpu().numpy()\n",
    "        \n",
    "        total_loss += F.binary_cross_entropy(res.squeeze(), data.node_label.to(torch.float))\n",
    "        auc += roc_auc_score(y, pred)\n",
    "        ap += average_precision_score(y, pred)\n",
    "        \n",
    "    return total_loss / len(test_dataset), auc / len(test_dataset), ap / len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f58ad428-cdd3-4997-a50a-8a911215f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(test_dataset):\n",
    "    model.eval()\n",
    "    \n",
    "    loss, auc, ap = [], [], []\n",
    "    for data in test_dataset:\n",
    "        data = data.to(device)\n",
    "        res = model(data.x, data.edge_index, None, None)\n",
    "        \n",
    "        y, pred = data.node_label.cpu().numpy(), res.squeeze().cpu().numpy()\n",
    "        \n",
    "        loss.apppend(F.binary_cross_entropy(res.squeeze(), data.node_label.to(torch.float)))\n",
    "        auc.append(roc_auc_score(y, pred))\n",
    "        ap.append(average_precision_score(y, pred))\n",
    "        \n",
    "    return loss, auc, ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40d1caae-4b52-4a14-a920-4fec3e93596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='runs/g2braph_batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc7b73-2132-4f35-97b1-b05117d34b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 2001):\n",
    "    train_loss = train(train_loader)\n",
    "    val_loss, val_auc, val_ap = validate(val_loader)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/validate', val_loss, epoch)\n",
    "    writer.add_scalar('AUC/validate', val_auc, epoch)\n",
    "    writer.add_scalar('AP/validate', val_ap, epoch)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f'{time.ctime()}  '\n",
    "              f'Epoch: {epoch:04d}, Train Loss: {train_loss:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}, '\n",
    "              f'Val AP: {val_ap:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b56a7-e2be-4976-a391-3001fdb66c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_auc, test_ap = test(test_dataset.shuffle())\n",
    "print('Test Loss: \\t',\n",
    "      ' '.join(f'{x:.6f}' for x in test_loss), '\\n'\n",
    "      'Test AUC: \\t',\n",
    "      ' '.join(f'{x:.6f}' for x in test_auc), '\\n'\n",
    "      'Test AP: \\t',\n",
    "      ' '.join(f'{x:.6f}' for x in test_ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d2d07-e5c8-4d76-8813-ec4d8b463f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep AI",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
